{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37dafa0a",
   "metadata": {},
   "source": [
    "## 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8509d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pickle\n",
    "from scipy import signal\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296080f6",
   "metadata": {},
   "source": [
    "## 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc215c6",
   "metadata": {},
   "source": [
    "Only run if retraining model with new data or such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28931031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WESAD dataset loader\n",
    "\n",
    "class WesadDataLoader:\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculateHrv(rrIntervals):\n",
    "        if len(rrIntervals) < 2:\n",
    "            return None, None, None, None\n",
    "        \n",
    "        rrIntervals = rrIntervals[(rrIntervals > 300) & (rrIntervals < 2000)]\n",
    "        \n",
    "        if len(rrIntervals) < 2:\n",
    "            return None, None, None, None\n",
    "        \n",
    "        sdnn = np.std(rrIntervals)\n",
    "        diffRr = np.diff(rrIntervals)\n",
    "        rmssd = np.sqrt(np.mean(diffRr ** 2))\n",
    "        meanHr = 60000 / np.mean(rrIntervals)  # heart rate\n",
    "        pnn50 = np.sum(np.abs(diffRr) > 50) / len(diffRr) * 100  # stress indicator\n",
    "        \n",
    "        return sdnn, rmssd, meanHr, pnn50\n",
    "    \n",
    "    @staticmethod\n",
    "    def getRrFromEcg(ecgSignal, fs=700):\n",
    "        nyq = fs / 2\n",
    "        low = 0.5 / nyq\n",
    "        high = 40 / nyq\n",
    "        b, a = signal.butter(4, [low, high], btype='band')\n",
    "        ecgFilt = signal.filtfilt(b, a, ecgSignal)\n",
    "        \n",
    "        ecgNorm = (ecgFilt - np.mean(ecgFilt)) / np.std(ecgFilt)\n",
    "        \n",
    "        minDist = int(0.4 * fs)\n",
    "        thresh = np.mean(ecgNorm) + 0.5 * np.std(ecgNorm)\n",
    "        peaks = signal.find_peaks(ecgNorm, height=thresh, distance=minDist)[0]\n",
    "        \n",
    "        if len(peaks) < 2:\n",
    "            return None\n",
    "        \n",
    "        rr = np.diff(peaks) / fs * 1000\n",
    "        \n",
    "        return rr\n",
    "    \n",
    "    @staticmethod\n",
    "    def loadWesadSubject(subjectPath):\n",
    "        with open(subjectPath, 'rb') as f:\n",
    "            data = pickle.load(f, encoding='latin1')\n",
    "        \n",
    "        if 'signal' not in data or 'chest' not in data['signal']:\n",
    "            return None\n",
    "        \n",
    "        chest = data['signal']['chest']\n",
    "        labels = data['label']\n",
    "        \n",
    "        if isinstance(chest, dict):\n",
    "            ecg = chest['ECG'].flatten() if 'ECG' in chest else None\n",
    "        else:\n",
    "            ecg = chest[:, 0] if len(chest.shape) > 1 else chest\n",
    "            \n",
    "        if ecg is None:\n",
    "            return None\n",
    "        \n",
    "        fs = 700\n",
    "        \n",
    "        return ecg, labels, fs\n",
    "    \n",
    "    @staticmethod\n",
    "    def processWesadData(wesadDirectory):\n",
    "        subjectFiles = glob.glob(f\"{wesadDirectory}/**/S*.pkl\", recursive=True)\n",
    "        \n",
    "        print(f\"\\nFound {len(subjectFiles)} WESAD subjects\")\n",
    "        \n",
    "        if len(subjectFiles) == 0:\n",
    "            return None\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        for subjFile in subjectFiles:\n",
    "            subj = Path(subjFile).stem\n",
    "            print(f\"Processing {subj}...\")\n",
    "            \n",
    "            result = WesadDataLoader.loadWesadSubject(subjFile)\n",
    "            if result is None:\n",
    "                continue\n",
    "            \n",
    "            ecg, labels, fs = result\n",
    "            \n",
    "            winSize = 60 * fs\n",
    "            step = winSize // 2\n",
    "            \n",
    "            for i in range((len(ecg) - winSize) // step + 1):\n",
    "                start = i * step\n",
    "                end = start + winSize\n",
    "                \n",
    "                if end > len(ecg):\n",
    "                    break\n",
    "                \n",
    "                ecgWin = ecg[start:end]\n",
    "                labelWin = labels[start:end]\n",
    "                \n",
    "                vals, counts = np.unique(labelWin, return_counts=True)\n",
    "                label = vals[np.argmax(counts)]\n",
    "                \n",
    "                if label == 0:\n",
    "                    continue\n",
    "                elif label == 1:\n",
    "                    stress = 0\n",
    "                elif label == 2:\n",
    "                    stress = 2\n",
    "                elif label == 3:\n",
    "                    stress = 1\n",
    "                elif label == 4:\n",
    "                    stress = 0\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                rr = WesadDataLoader.getRrFromEcg(ecgWin, fs)\n",
    "                \n",
    "                if rr is None or len(rr) < 10:\n",
    "                    continue\n",
    "                \n",
    "                sdnn, rmssd, meanHr, pnn50 = WesadDataLoader.calculateHrv(rr)\n",
    "                \n",
    "                if sdnn is None:\n",
    "                    continue\n",
    "                \n",
    "                features.append({\n",
    "                    'subject': subj,\n",
    "                    'sdnn': sdnn,\n",
    "                    'rmssd': rmssd,\n",
    "                    'meanHr': meanHr,\n",
    "                    'pnn50': pnn50,\n",
    "                    'stressClass': stress,\n",
    "                    'source': 'WESAD'\n",
    "                })\n",
    "        \n",
    "        df = pd.DataFrame(features)\n",
    "        \n",
    "        print(f\"\\nWESAD data:\")\n",
    "        print(f\"  Total samples: {len(df)}\")\n",
    "        print(f\"  Low: {(df['stressClass'] == 0).sum()}\")\n",
    "        print(f\"  Moderate: {(df['stressClass'] == 1).sum()}\")\n",
    "        print(f\"  High: {(df['stressClass'] == 2).sum()}\")\n",
    "        print(f\"  SDNN range: {df['sdnn'].min():.1f} - {df['sdnn'].max():.1f}\")\n",
    "        print(f\"  RMSSD range: {df['rmssd'].min():.1f} - {df['rmssd'].max():.1f}\")\n",
    "        \n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7e3c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samsung health data loader\n",
    "\n",
    "class SamsungDataLoader:\n",
    "    \n",
    "    @staticmethod\n",
    "    def loadAllSamsungFiles(dataDirectory):\n",
    "        hrvFiles = glob.glob(dataDirectory + '/com.samsung.health.hrv/**/*.json', recursive=True)\n",
    "        stressFiles = glob.glob(dataDirectory + '/com.samsung.shealth.stress/**/*.json', recursive=True)\n",
    "        \n",
    "        print(f\"Found:\")\n",
    "        print(f\"  Stress files: {len(stressFiles)}\")\n",
    "        print(f\"  HRV files: {len(hrvFiles)}\")\n",
    "        \n",
    "        stressData = []\n",
    "        for file in stressFiles:\n",
    "            with open(file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    stressData.extend(data)\n",
    "                else:\n",
    "                    stressData.append(data)\n",
    "        \n",
    "        hrvData = []\n",
    "        for file in hrvFiles:\n",
    "            if 'stress' in file.lower() or 'heart_rate' in file.lower():\n",
    "                continue\n",
    "            with open(file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    hrvData.extend(data)\n",
    "                else:\n",
    "                    hrvData.append(data)\n",
    "        \n",
    "        return stressData, hrvData\n",
    "    \n",
    "    @staticmethod\n",
    "    def processStressData(stressData):\n",
    "        df = pd.DataFrame(stressData)\n",
    "        df['timestamp'] = pd.to_datetime(df['start_time'], unit='ms')\n",
    "        \n",
    "        df['stressNormalized'] = df['score'] / df['score'].max()\n",
    "        \n",
    "        lowThresh = df['score'].quantile(0.33)\n",
    "        highThresh = df['score'].quantile(0.66)\n",
    "        \n",
    "        df['stressClass'] = 1\n",
    "        df.loc[df['score'] <= lowThresh, 'stressClass'] = 0\n",
    "        df.loc[df['score'] > highThresh, 'stressClass'] = 2\n",
    "        \n",
    "        print(f\"Stress data:\")\n",
    "        print(f\"  Samples: {len(df)}\")\n",
    "        print(f\"  Score range: {df['score'].min():.1f} - {df['score'].max():.1f}\")\n",
    "        print(f\"  Low stress: {(df['stressClass'] == 0).sum()}\")\n",
    "        print(f\"  Moderate stress: {(df['stressClass'] == 1).sum()}\")\n",
    "        print(f\"  High stress: {(df['stressClass'] == 2).sum()}\")\n",
    "        \n",
    "        return df[['timestamp', 'score', 'stressClass', 'stressNormalized']]\n",
    "    \n",
    "    @staticmethod\n",
    "    def processHrvData(hrvData):\n",
    "        df = pd.DataFrame(hrvData)\n",
    "        df['timestamp'] = pd.to_datetime(df['start_time'], unit='ms')\n",
    "        \n",
    "        if 'sdnn' not in df.columns:\n",
    "            df['sdnn'] = np.nan\n",
    "        if 'rmssd' not in df.columns:\n",
    "            df['rmssd'] = np.nan\n",
    "        \n",
    "        print(f\"HRV data:\")\n",
    "        print(f\"  Samples: {len(df)}\")\n",
    "        print(f\"  SDNN range: {df['sdnn'].min():.1f} - {df['sdnn'].max():.1f}\")\n",
    "        print(f\"  RMSSD range: {df['rmssd'].min():.1f} - {df['rmssd'].max():.1f}\")\n",
    "        \n",
    "        return df[['timestamp', 'sdnn', 'rmssd']]\n",
    "    \n",
    "    @staticmethod\n",
    "    def mergeStressAndHrv(dfStress, dfHrv):\n",
    "        dfStress['timestampRounded'] = dfStress['timestamp'].dt.round('1min')\n",
    "        dfHrv['timestampRounded'] = dfHrv['timestamp'].dt.round('1min')\n",
    "        \n",
    "        merged = pd.merge_asof(\n",
    "            dfHrv.sort_values('timestamp'),\n",
    "            dfStress.sort_values('timestamp'),\n",
    "            on='timestamp',\n",
    "            direction='nearest',\n",
    "            tolerance=pd.Timedelta('1hr')\n",
    "        )\n",
    "        \n",
    "        merged = merged.dropna(subset=['stressClass'])\n",
    "        merged = merged.dropna(subset=['sdnn', 'rmssd'])\n",
    "        \n",
    "        print(f\"Merged data:\")\n",
    "        print(f\"  Matched samples: {len(merged)}\")\n",
    "        print(f\"  Time span: {merged['timestamp'].min()} to {merged['timestamp'].max()}\")\n",
    "        print(f\"  Duration: {(merged['timestamp'].max() - merged['timestamp'].min()).days} days\")\n",
    "        \n",
    "        return merged[['timestamp', 'sdnn', 'rmssd', 'stressClass', 'score']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea412091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training class\n",
    "\n",
    "class StressClassifier:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = None\n",
    "        self.features = ['sdnn', 'rmssd', 'meanHr', 'pnn50']  # add new features\n",
    "    \n",
    "    def train(self, X, y, testSize=0.2):\n",
    "        xTrain, xTest, yTrain, yTest = train_test_split(\n",
    "            X, y, test_size=testSize, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        xTrainScaled = self.scaler.fit_transform(xTrain)\n",
    "        xTestScaled = self.scaler.transform(xTest)\n",
    "        \n",
    "        # calculate sample weights\n",
    "        #from sklearn.utils.class_weight import compute_sample_weight\n",
    "        #sampleWeights = compute_sample_weight('balanced', yTrain)\n",
    "        from sklearn.utils.class_weight import compute_class_weight\n",
    "        classWeights = compute_class_weight('balanced', classes=np.unique(yTrain), y=yTrain)\n",
    "        classWeightDict = {0: classWeights[0], 1: classWeights[1]*2, 2: classWeights[2]}\n",
    "        sampleWeights = np.array([classWeightDict[y] for y in yTrain])\n",
    "        self.model = xgb.XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            num_class=3\n",
    "        )\n",
    "        \n",
    "        # add sample_weight here\n",
    "        self.model.fit(xTrainScaled, yTrain, sample_weight=sampleWeights, verbose=False)\n",
    "\n",
    "        \n",
    "        yPred = self.model.predict(xTestScaled)\n",
    "        yProba = self.model.predict_proba(xTestScaled)\n",
    "        \n",
    "        print(\"\\nEVALUATION\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        labels = ['Low', 'Moderate', 'High']\n",
    "        print(classification_report(yTest, yPred, target_names=labels))\n",
    "        \n",
    "        print(f\"\\nConfusion Matrix:\")\n",
    "        print(confusion_matrix(yTest, yPred))\n",
    "        \n",
    "        auc = roc_auc_score(yTest, yProba, multi_class='ovr', average='macro')\n",
    "        print(f\"\\nROC-AUC: {auc:.3f}\")\n",
    "        \n",
    "        print(\"\\nFeature importance:\")\n",
    "        for f, imp in zip(self.features, self.model.feature_importances_):\n",
    "            print(f\"  {f}: {imp:.3f}\")\n",
    "        \n",
    "        cm = confusion_matrix(yTest, yPred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(cm, cmap='Blues')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xticks([0,1,2], labels)\n",
    "        plt.yticks([0,1,2], labels)\n",
    "        \n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                plt.text(j, i, cm[i,j], ha='center', va='center')\n",
    "        \n",
    "        plt.savefig('confusion.png', dpi=150)\n",
    "        plt.close()\n",
    "        print(\"\\nSaved confusion.png\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def predict(self, xNew):\n",
    "        xScaled = self.scaler.transform(xNew)\n",
    "        preds = self.model.predict(xScaled)\n",
    "        proba = self.model.predict_proba(xScaled)\n",
    "        return preds, proba\n",
    "    \n",
    "    def saveModel(self, modelFile='stress_model.pkl', scalerFile='scaler.pkl'):\n",
    "        joblib.dump(self.model, modelFile)\n",
    "        joblib.dump(self.scaler, scalerFile)\n",
    "        print(f\"\\nSaved {modelFile} and {scalerFile}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e6286a",
   "metadata": {},
   "source": [
    "## 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d347e9a1",
   "metadata": {},
   "source": [
    "Train New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b838bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main script\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Stress Classification - 3 Classes\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    samsungDir = r'D:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver2\\HRVModule\\XGBoost\\jsons'\n",
    "    wesadDir = r'D:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver2\\HRVModule\\XGBoost\\WESAD'\n",
    "    \n",
    "    useSamsung = False\n",
    "    useWesad = True\n",
    "    \n",
    "    allData = []\n",
    "    \n",
    "    if useSamsung:\n",
    "        print(\"\\nLoading Samsung data...\")\n",
    "        loader = SamsungDataLoader()\n",
    "        stressData, hrvData = loader.loadAllSamsungFiles(samsungDir)\n",
    "        \n",
    "        if stressData and hrvData:\n",
    "            dfStress = loader.processStressData(stressData)\n",
    "            dfHrv = loader.processHrvData(hrvData)\n",
    "            dfSamsung = loader.mergeStressAndHrv(dfStress, dfHrv)\n",
    "            \n",
    "            dfSamsung['source'] = 'Samsung'\n",
    "            allData.append(dfSamsung[['sdnn', 'rmssd', 'stressClass', 'source']])\n",
    "            print(f\"Added {len(dfSamsung)} Samsung samples\")\n",
    "    \n",
    "    if useWesad:\n",
    "        print(\"\\nLoading WESAD data...\")\n",
    "        wesadLoader = WesadDataLoader()\n",
    "        dfWesad = wesadLoader.processWesadData(wesadDir)\n",
    "        \n",
    "        if dfWesad is not None and len(dfWesad) > 0:\n",
    "            allData.append(dfWesad[['sdnn', 'rmssd', 'meanHr', 'pnn50', 'stressClass', 'source']])    \n",
    "    print(\"\\nCombining datasets...\")\n",
    "    df = pd.concat(allData, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\nTotal samples: {len(df)}\")\n",
    "    for src in df['source'].unique():\n",
    "        srcDf = df[df['source'] == src]\n",
    "        print(f\"\\n{src}:\")\n",
    "        print(f\"  Samples: {len(srcDf)}\")\n",
    "        print(f\"  Low: {(srcDf['stressClass'] == 0).sum()}\")\n",
    "        print(f\"  Moderate: {(srcDf['stressClass'] == 1).sum()}\")\n",
    "        print(f\"  High: {(srcDf['stressClass'] == 2).sum()}\")\n",
    "    \n",
    "    print(\"\\nTraining model...\")\n",
    "    X = df[['sdnn', 'rmssd', 'meanHr', 'pnn50']].values\n",
    "    y = df['stressClass'].values\n",
    "    \n",
    "    clf = StressClassifier()\n",
    "    clf.train(X, y)\n",
    "    clf.saveModel()\n",
    "    \n",
    "    print(\"\\nDONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ab50c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for model files...\n",
      "Found model at: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver1 on git\\HRVModule\\X2\\XGBoost\\stress_model.pkl\n",
      "Found scaler at: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver1 on git\\HRVModule\\X2\\XGBoost\\scaler.pkl\n",
      "Found model at: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver1 on git\\HRVModule\\X2\\XGBoost\\Others\\stress_model.pkl\n",
      "Found scaler at: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver1 on git\\HRVModule\\X2\\XGBoost\\Others\\scaler.pkl\n",
      "Found model at: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver1 on git\\HRVModule\\XGBoost\\stress_model.pkl\n",
      "Found scaler at: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver1 on git\\HRVModule\\XGBoost\\scaler.pkl\n",
      "Found model at: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver1 on git\\HRVModule\\XGBoost\\Others\\stress_model.pkl\n",
      "Found scaler at: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver1 on git\\HRVModule\\XGBoost\\Others\\scaler.pkl\n",
      "Found model at: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver2\\HRVModule\\XGBoost\\stress_model.pkl\n",
      "Found scaler at: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver2\\HRVModule\\XGBoost\\scaler.pkl\n",
      "Found model at: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver2\\HRVModule\\XGBoost\\Others\\stress_model.pkl\n",
      "Found scaler at: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver2\\HRVModule\\XGBoost\\Others\\scaler.pkl\n",
      "Found model at: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver2 - Copy\\HealMind_Ver3\\stress_model.pkl\n",
      "Found scaler at: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver2 - Copy\\HealMind_Ver3\\scaler.pkl\n",
      "Found model at: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver2 - Copy\\HealMind_Ver3\\HRVModule\\XG_Calc_predict\\stress_model.pkl\n",
      "Found scaler at: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver2 - Copy\\HealMind_Ver3\\HRVModule\\XG_Calc_predict\\scaler.pkl\n",
      "Found model at: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver2 - Copy\\HealMind_Ver3 - Copy\\HRVModule\\XGBoost\\stress_model.pkl\n",
      "Found scaler at: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver2 - Copy\\HealMind_Ver3 - Copy\\HRVModule\\XGBoost\\scaler.pkl\n",
      "Found model at: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver2 - Copy\\HealMind_Ver3 - Copy\\HRVModule\\XGBoost\\Others\\stress_model.pkl\n",
      "Found scaler at: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver2 - Copy\\HealMind_Ver3 - Copy\\HRVModule\\XGBoost\\Others\\scaler.pkl\n",
      "Loading model...\n",
      "Model loaded\n",
      "Loading scaler...\n",
      "Scaler loaded\n",
      "All files loaded successfully\n",
      "\n",
      "============================================================\n",
      "MODEL FILE LOCATIONS:\n",
      "============================================================\n",
      "Model file: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver2 - Copy\\HealMind_Ver3 - Copy\\HRVModule\\XGBoost\\Others\\stress_model.pkl\n",
      "Scaler file: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver2 - Copy\\HealMind_Ver3 - Copy\\HRVModule\\XGBoost\\Others\\scaler.pkl\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Looking for model files...\")\n",
    "\n",
    "# find model files\n",
    "startPath = os.path.abspath('../../')\n",
    "modelPath = None\n",
    "scalerPath = None\n",
    "\n",
    "for root, dirs, files in os.walk(startPath):\n",
    "    if 'stress_model.pkl' in files:\n",
    "        modelPath = os.path.join(root, 'stress_model.pkl')\n",
    "        print(f\"Found model at: {modelPath}\")\n",
    "    if 'scaler.pkl' in files:\n",
    "        scalerPath = os.path.join(root, 'scaler.pkl')\n",
    "        print(f\"Found scaler at: {scalerPath}\")\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = joblib.load(modelPath)\n",
    "print(\"Model loaded\")\n",
    "\n",
    "print(\"Loading scaler...\")\n",
    "scaler = joblib.load(scalerPath)\n",
    "print(\"Scaler loaded\")\n",
    "\n",
    "print(\"All files loaded successfully\")\n",
    "\n",
    "# show file locations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL FILE LOCATIONS:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model file: {modelPath}\")\n",
    "print(f\"Scaler file: {scalerPath}\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a919fb",
   "metadata": {},
   "source": [
    "## 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bdd841",
   "metadata": {},
   "source": [
    "Predictor; Need to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "24fc213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#StressPredictor Class\n",
    "\n",
    "class StressPredictor:\n",
    "    \n",
    "    def __init__(self, modelFile='stress_model.pkl', scalerFile='scaler.pkl', db=None):\n",
    "        self.model = joblib.load(modelFile)\n",
    "        self.scaler = joblib.load(scalerFile)\n",
    "        self.db = db\n",
    "        \n",
    "        self.minConfidence = 0.60\n",
    "        self.minSdnn = 10\n",
    "        self.maxSdnn = 200\n",
    "        self.minRmssd = 5\n",
    "        self.maxRmssd = 150\n",
    "        \n",
    "        \n",
    "        print(\"Model loaded\")\n",
    "    \n",
    "    def predictSingle(self, sdnn, rmssd, meanHr, pnn50, showDetails=True):\n",
    "        if not (self.minSdnn <= sdnn <= self.maxSdnn):\n",
    "            return {'error': f'Invalid SDNN: {sdnn:.2f}'}\n",
    "        if not (self.minRmssd <= rmssd <= self.maxRmssd):\n",
    "            return {'error': f'Invalid RMSSD: {rmssd:.2f}'}\n",
    "        \n",
    "        X = np.array([[sdnn, rmssd, meanHr, pnn50]])\n",
    "        xScaled = self.scaler.transform(X)\n",
    "        \n",
    "        pred = self.model.predict(xScaled)[0]\n",
    "        proba = self.model.predict_proba(xScaled)[0]\n",
    "        \n",
    "        stressLabels = {0: 'LOW', 1: 'MODERATE', 2: 'HIGH'}\n",
    "        confidence = proba.max()\n",
    "        \n",
    "        result = {\n",
    "            'stressLevel': int(pred),\n",
    "            'stressLabel': stressLabels[pred],\n",
    "            'confidence': float(confidence),\n",
    "            'showToUser': confidence >= self.minConfidence,\n",
    "            'probabilities': {\n",
    "                'class_0_low': float(proba[0]),\n",
    "                'class_1_medium': float(proba[1]),\n",
    "                'class_2_high': float(proba[2])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if showDetails:\n",
    "            print(f\"\\nInput: SDNN={sdnn:.2f}, RMSSD={rmssd:.2f}, MeanHR={meanHr:.2f}, PNN50={pnn50:.2f}\")\n",
    "            print(f\"Predicted: {result['stressLabel']} (confidence: {confidence:.1%})\")\n",
    "            print(f\"Show to user: {'YES' if result['showToUser'] else 'NO'}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def calculateHrvFromIbi(self, ibiData):\n",
    "        if len(ibiData) < 2:\n",
    "            return None\n",
    "        \n",
    "        ibi = np.array(ibiData)\n",
    "        \n",
    "        ibi = ibi[(ibi > 300) & (ibi < 2000)]\n",
    "        \n",
    "        if len(ibi) < 2:\n",
    "            return None\n",
    "        \n",
    "        sdnn = np.std(ibi)\n",
    "        rmssd = np.sqrt(np.mean(np.diff(ibi) ** 2))\n",
    "        meanHr = 60000 / np.mean(ibi)\n",
    "        diffIbi = np.diff(ibi)\n",
    "        pnn50 = np.sum(np.abs(diffIbi) > 50) / len(diffIbi) * 100 if len(diffIbi) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'sdnn': sdnn,\n",
    "            'rmssd': rmssd,\n",
    "            'meanHr': meanHr,\n",
    "            'pnn50': pnn50\n",
    "        }\n",
    "    \n",
    "    def runBatch(self, hours=1):\n",
    "        if self.db is None:\n",
    "            print(\"No Firebase connection\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"BATCH PROCESSING - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        cutoff = datetime.now() - timedelta(hours=hours)\n",
    "        print(f\"Processing data from last {hours} hour(s)\")\n",
    "        print(f\"Cutoff time: {cutoff.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        try:\n",
    "            wearableRef = self.db.collection('heart_rate_data')\n",
    "            \n",
    "            docs = wearableRef.where('timestamp', '>=', cutoff).stream()\n",
    "            \n",
    "            processed = 0\n",
    "            saved = 0\n",
    "            errors = 0\n",
    "            \n",
    "            for doc in docs:\n",
    "                try:\n",
    "                    data = doc.to_dict()\n",
    "                    \n",
    "                    if 'ibi' in data and isinstance(data['ibi'], list) and len(data['ibi']) > 0:\n",
    "                        hrv = self.calculateHrvFromIbi(data['ibi'])\n",
    "                        \n",
    "                        if hrv is None:\n",
    "                            errors += 1\n",
    "                            continue\n",
    "                        \n",
    "                        prediction = self.predictSingle(\n",
    "                            hrv['sdnn'], \n",
    "                            hrv['rmssd'], \n",
    "                            hrv['meanHr'], \n",
    "                            hrv['pnn50'],\n",
    "                            showDetails=False\n",
    "                        )\n",
    "                    \n",
    "                    elif all(k in data for k in ['sdnn', 'rmssd', 'meanHr', 'pnn50']):\n",
    "                        prediction = self.predictSingle(\n",
    "                            data['sdnn'],\n",
    "                            data['rmssd'],\n",
    "                            data['meanHr'],\n",
    "                            data['pnn50'],\n",
    "                            showDetails=False\n",
    "                        )\n",
    "                    \n",
    "                    elif 'heart_rate' in data:\n",
    "                        hr = data['heart_rate']\n",
    "                        \n",
    "                        if hr < 70:\n",
    "                            sdnn, rmssd, pnn50 = 80, 65, 30\n",
    "                        elif hr < 90:\n",
    "                            sdnn, rmssd, pnn50 = 48, 38, 15\n",
    "                        else:\n",
    "                            sdnn, rmssd, pnn50 = 15, 10, 5\n",
    "                        \n",
    "                        prediction = self.predictSingle(\n",
    "                            sdnn, rmssd, hr, pnn50,\n",
    "                            showDetails=False\n",
    "                        )\n",
    "                    else:\n",
    "                        errors += 1\n",
    "                        continue\n",
    "                    \n",
    "                    if 'error' in prediction:\n",
    "                        errors += 1\n",
    "                        continue\n",
    "                    \n",
    "                    self.db.collection('stress_predictions2').add({\n",
    "                        'userId': 'QOyZROlPzUf25tKPvv0FWnd3NZw2',\n",
    "                        'userEmail': 'test@email.com',\n",
    "                        'stress_probabilities': prediction['probabilities'],\n",
    "                        'stressLevel': prediction['stressLevel'],\n",
    "                        'stressLabel': prediction['stressLabel'],\n",
    "                        'confidence': prediction['confidence'],\n",
    "                        'prediction_timestamp': firestore.SERVER_TIMESTAMP,\n",
    "                        'source': 'batch_prediction',\n",
    "                        'source_doc_id': doc.id\n",
    "                    })\n",
    "                    \n",
    "                    saved += 1\n",
    "                    processed += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    errors += 1\n",
    "                    print(f\"Error processing document {doc.id}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"\\nBATCH RESULTS:\")\n",
    "            print(f\"  Processed: {processed}\")\n",
    "            print(f\"  Saved: {saved}\")\n",
    "            print(f\"  Errors: {errors}\")\n",
    "            print(f\"{'='*60}\\n\")\n",
    "            \n",
    "            return {\n",
    "                'processed': processed,\n",
    "                'saved': saved,\n",
    "                'errors': errors,\n",
    "                'timestamp': datetime.now()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Batch processing failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def fetchAndPredictFromFirebase(self, hours=1):\n",
    "        return self.runBatch(hours=hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b7ddf",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d090cf",
   "metadata": {},
   "source": [
    "Firebase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a630158",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stress Prediction System - NO USER ID REQUIRED\n",
    "Processes ALL heart_rate_data without user filtering\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class StressPredictor:\n",
    "    \"\"\"Stress predictor - processes ALL data regardless of user\"\"\"\n",
    "    \n",
    "    def __init__(self, model_file, scaler_file, db, default_user_id='QOyZROlPzUf25tKPvv0FWnd3NZw2', default_email='user@healmind.com'):\n",
    "        self.model = joblib.load(model_file)\n",
    "        self.scaler = joblib.load(scaler_file)\n",
    "        self.db = db\n",
    "        self.default_user_id = default_user_id\n",
    "        self.default_email = default_email\n",
    "        print(\"Model and scaler loaded successfully\")\n",
    "        print(f\"Default user: {default_email} (ID: {default_user_id})\")\n",
    "    \n",
    "    def calculate_hrv_from_ibi(self, ibi_data):\n",
    "        \"\"\"Calculate HRV metrics from IBI data\"\"\"\n",
    "        if not isinstance(ibi_data, list) or len(ibi_data) < 2:\n",
    "            return None\n",
    "        \n",
    "        ibi = np.array(ibi_data)\n",
    "        \n",
    "        # Filter valid IBI values (300-2000ms)\n",
    "        ibi = ibi[(ibi > 300) & (ibi < 2000)]\n",
    "        \n",
    "        if len(ibi) < 2:\n",
    "            return None\n",
    "        \n",
    "        # Calculate HRV metrics (4 features to match model training)\n",
    "        rmssd = np.sqrt(np.mean(np.diff(ibi) ** 2))\n",
    "        sdnn = np.std(ibi)\n",
    "        mean_hr = 60000 / np.mean(ibi) if np.mean(ibi) > 0 else 0\n",
    "        \n",
    "        # Calculate pNN50 (4th feature) - percentage of successive RR intervals that differ by more than 50ms\n",
    "        diff_ibi = np.abs(np.diff(ibi))\n",
    "        pnn50 = (np.sum(diff_ibi > 50) / len(diff_ibi) * 100) if len(diff_ibi) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'rmssd': float(rmssd),\n",
    "            'sdnn': float(sdnn),\n",
    "            'mean_hr': float(mean_hr),\n",
    "            'pnn50': float(pnn50)\n",
    "        }\n",
    "    \n",
    "    def predict_stress(self, hrv_metrics):\n",
    "        \"\"\"Predict stress level from HRV metrics\"\"\"\n",
    "        if hrv_metrics is None:\n",
    "            return None\n",
    "        \n",
    "        # Prepare features (4 features to match model training)\n",
    "        features = np.array([[\n",
    "            hrv_metrics['rmssd'],\n",
    "            hrv_metrics['sdnn'],\n",
    "            hrv_metrics['mean_hr'],\n",
    "            hrv_metrics['pnn50']\n",
    "        ]])\n",
    "        \n",
    "        # Scale and predict\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        probabilities = self.model.predict_proba(features_scaled)[0]\n",
    "        prediction = self.model.predict(features_scaled)[0]\n",
    "        \n",
    "        stress_labels = ['LOW', 'MEDIUM', 'HIGH']\n",
    "        \n",
    "        return {\n",
    "            'stressLevel': int(prediction),\n",
    "            'stressLabel': stress_labels[int(prediction)],\n",
    "            'confidence': float(max(probabilities)),\n",
    "            'probabilities': {\n",
    "                'class_0_low': float(probabilities[0]),\n",
    "                'class_1_medium': float(probabilities[1]),\n",
    "                'class_2_high': float(probabilities[2])\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def run_batch(self, hours=24, verbose=True):\n",
    "        \"\"\"Process ALL documents from heart_rate_data - NO USER FILTER\"\"\"\n",
    "        cutoff = datetime.now() - timedelta(hours=hours)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nBATCH PROCESSING - ALL DATA\")\n",
    "            print(f\"Time window: Last {hours} hours\")\n",
    "            print(f\"Cutoff: {cutoff}\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Get ALL documents - NO userId filter\n",
    "            print(f\"Querying heart_rate_data collection...\")\n",
    "            docs = self.db.collection('heart_rate_data')\\\n",
    "                .where('timestamp', '>=', cutoff)\\\n",
    "                .stream()\n",
    "            \n",
    "            docs_list = list(docs)\n",
    "            total_docs = len(docs_list)\n",
    "            \n",
    "            print(f\"Found {total_docs} documents\\n\")\n",
    "            \n",
    "            if total_docs == 0:\n",
    "                print(f\"NO DOCUMENTS FOUND in the last {hours} hours!\")\n",
    "                return {'processed': 0, 'skipped': 0, 'errors': 0, 'total': 0}\n",
    "            \n",
    "            processed = 0\n",
    "            skipped = 0\n",
    "            errors = 0\n",
    "            \n",
    "            for idx, doc in enumerate(docs_list, 1):\n",
    "                try:\n",
    "                    data = doc.to_dict()\n",
    "                    \n",
    "                    # Extract IBI data\n",
    "                    ibi_data = data.get('ibi', [])\n",
    "                    \n",
    "                    if not ibi_data:\n",
    "                        skipped += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate HRV\n",
    "                    hrv_metrics = self.calculate_hrv_from_ibi(ibi_data)\n",
    "                    \n",
    "                    if hrv_metrics is None:\n",
    "                        skipped += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Predict stress\n",
    "                    prediction = self.predict_stress(hrv_metrics)\n",
    "                    \n",
    "                    if prediction is None:\n",
    "                        skipped += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Save prediction - use default userId and email\n",
    "                    self.db.collection('stress_predictions2').add({\n",
    "                        'userId': self.default_user_id,\n",
    "                        'userEmail': self.default_email,\n",
    "                        'stress_probabilities': prediction['probabilities'],\n",
    "                        'stressLevel': prediction['stressLevel'],\n",
    "                        'stressLabel': prediction['stressLabel'],\n",
    "                        'confidence': prediction['confidence'],\n",
    "                        'prediction_timestamp': firestore.SERVER_TIMESTAMP,\n",
    "                        'source': 'batch_prediction',\n",
    "                        'source_doc_id': doc.id,\n",
    "                        'hrv_metrics': hrv_metrics\n",
    "                    })\n",
    "                    \n",
    "                    processed += 1\n",
    "                    \n",
    "                    # Progress indicator\n",
    "                    if verbose and processed % 50 == 0:\n",
    "                        print(f\"  Progress: {idx}/{total_docs} docs | {processed} processed | {skipped} skipped\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if verbose:\n",
    "                        print(f\"  Error on doc {doc.id}: {str(e)}\")\n",
    "                    errors += 1\n",
    "            \n",
    "            result = {\n",
    "                'processed': processed,\n",
    "                'skipped': skipped,\n",
    "                'errors': errors,\n",
    "                'total': total_docs\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nBATCH PROCESSING COMPLETE\")\n",
    "            print(f\"  Successfully processed: {processed}\")\n",
    "            print(f\"  Skipped (insufficient data): {skipped}\")\n",
    "            print(f\"  Errors: {errors}\")\n",
    "            print(f\"  Total documents checked: {total_docs}\")\n",
    "            print(f\"  Success rate: {(processed/total_docs*100):.1f}%\\n\")\n",
    "            \n",
    "            if processed > 0:\n",
    "                print(f\"{processed} predictions saved to 'stress_predictions2' collection!\")\n",
    "            else:\n",
    "                print(f\"No predictions were saved. Check the skipped count above.\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nFATAL ERROR in batch processing\")\n",
    "            print(f\"   Error: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return {'error': str(e)}\n",
    "\n",
    "\n",
    "def quick_data_check(db, hours=24):\n",
    "    \"\"\"Quick check of what's in the database\"\"\"\n",
    "    print(f\"\\nQUICK DATA CHECK\\n\")\n",
    "    \n",
    "    cutoff = datetime.now() - timedelta(hours=hours)\n",
    "    \n",
    "    # Check heart_rate_data\n",
    "    print(f\"Checking heart_rate_data (last {hours} hours)...\")\n",
    "    docs = db.collection('heart_rate_data').where('timestamp', '>=', cutoff).stream()\n",
    "    docs_list = list(docs)\n",
    "    \n",
    "    print(f\"   Total documents: {len(docs_list)}\")\n",
    "    \n",
    "    if len(docs_list) > 0:\n",
    "        # Count documents with valid IBI data\n",
    "        valid_ibi = 0\n",
    "        total_ibi_values = 0\n",
    "        \n",
    "        for doc in docs_list:\n",
    "            data = doc.to_dict()\n",
    "            ibi = data.get('ibi', [])\n",
    "            if isinstance(ibi, list) and len(ibi) >= 2:\n",
    "                valid_ibi += 1\n",
    "                total_ibi_values += len(ibi)\n",
    "        \n",
    "        print(f\"   Documents with valid IBI (â‰¥2 values): {valid_ibi}\")\n",
    "        print(f\"   Total IBI values: {total_ibi_values}\")\n",
    "        print(f\"   Average IBI per doc: {total_ibi_values/len(docs_list):.1f}\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(f\"\\n   Sample document:\")\n",
    "        sample = docs_list[0].to_dict()\n",
    "        print(f\"      Fields: {list(sample.keys())}\")\n",
    "        print(f\"      IBI count: {len(sample.get('ibi', []))}\")\n",
    "        if sample.get('ibi'):\n",
    "            print(f\"      IBI sample: {sample['ibi'][:10]}\")\n",
    "    \n",
    "    # Check stress_predictions2\n",
    "    print(f\"\\nChecking stress_predictions2...\")\n",
    "    pred_docs = db.collection('stress_predictions2').stream()\n",
    "    pred_docs_list = list(pred_docs)\n",
    "    \n",
    "    print(f\"   Total predictions: {len(pred_docs_list)}\")\n",
    "    \n",
    "    if len(pred_docs_list) > 0:\n",
    "        recent = pred_docs_list[-1].to_dict()\n",
    "        print(f\"\\n   Most recent prediction:\")\n",
    "        print(f\"      Stress level: {recent.get('stressLabel', 'N/A')}\")\n",
    "        print(f\"      Confidence: {recent.get('confidence', 'N/A'):.1%}\")\n",
    "        print(f\"      Source: {recent.get('source', 'N/A')}\")\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f0c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stress Prediction System - NO USER ID REQUIRED\n",
    "Processes ALL heart_rate_data without user filtering\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class StressPredictor:\n",
    "    \"\"\"Stress predictor - processes ALL data regardless of user\"\"\"\n",
    "    \n",
    "    def __init__(self, model_file, scaler_file, db, default_user_id='QOyZROlPzUf25tKPvv0FWnd3NZw2', default_email='user@healmind.com'):\n",
    "        self.model = joblib.load(model_file)\n",
    "        self.scaler = joblib.load(scaler_file)\n",
    "        self.db = db\n",
    "        self.default_user_id = default_user_id\n",
    "        self.default_email = default_email\n",
    "        print(\"Model and scaler loaded successfully\")\n",
    "        print(f\"Default user: {default_email} (ID: {default_user_id})\")\n",
    "    \n",
    "    def calculate_hrv_from_ibi(self, ibi_data):\n",
    "        \"\"\"Calculate HRV metrics from IBI data\"\"\"\n",
    "        if not isinstance(ibi_data, list) or len(ibi_data) < 2:\n",
    "            return None\n",
    "        \n",
    "        ibi = np.array(ibi_data)\n",
    "        \n",
    "        # Filter valid IBI values (300-2000ms)\n",
    "        ibi = ibi[(ibi > 300) & (ibi < 2000)]\n",
    "        \n",
    "        if len(ibi) < 2:\n",
    "            return None\n",
    "        \n",
    "        # Calculate HRV metrics (4 features to match model training)\n",
    "        rmssd = np.sqrt(np.mean(np.diff(ibi) ** 2))\n",
    "        sdnn = np.std(ibi)\n",
    "        mean_hr = 60000 / np.mean(ibi) if np.mean(ibi) > 0 else 0\n",
    "        \n",
    "        # Calculate pNN50 (4th feature) - percentage of successive RR intervals that differ by more than 50ms\n",
    "        diff_ibi = np.abs(np.diff(ibi))\n",
    "        pnn50 = (np.sum(diff_ibi > 50) / len(diff_ibi) * 100) if len(diff_ibi) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'rmssd': float(rmssd),\n",
    "            'sdnn': float(sdnn),\n",
    "            'mean_hr': float(mean_hr),\n",
    "            'pnn50': float(pnn50)\n",
    "        }\n",
    "    \n",
    "    def predict_stress(self, hrv_metrics):\n",
    "        \"\"\"Predict stress level from HRV metrics\"\"\"\n",
    "        if hrv_metrics is None:\n",
    "            return None\n",
    "        \n",
    "        # Prepare features (4 features to match model training)\n",
    "        features = np.array([[\n",
    "            hrv_metrics['rmssd'],\n",
    "            hrv_metrics['sdnn'],\n",
    "            hrv_metrics['mean_hr'],\n",
    "            hrv_metrics['pnn50']\n",
    "        ]])\n",
    "        \n",
    "        # Scale and predict\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        probabilities = self.model.predict_proba(features_scaled)[0]\n",
    "        prediction = self.model.predict(features_scaled)[0]\n",
    "        \n",
    "        stress_labels = ['LOW', 'MEDIUM', 'HIGH']\n",
    "        \n",
    "        return {\n",
    "            'stressLevel': int(prediction),\n",
    "            'stressLabel': stress_labels[int(prediction)],\n",
    "            'confidence': float(max(probabilities)),\n",
    "            'probabilities': {\n",
    "                'class_0_low': float(probabilities[0]),\n",
    "                'class_1_medium': float(probabilities[1]),\n",
    "                'class_2_high': float(probabilities[2])\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def run_batch(self, hours=24, verbose=True):\n",
    "        \"\"\"Process ALL documents from heart_rate_data - NO USER FILTER\"\"\"\n",
    "        cutoff = datetime.now() - timedelta(hours=hours)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n---------------------------------------------------------------\")\n",
    "            print(f\"BATCH PROCESSING - ALL DATA\")\n",
    "            print(f\"Time window: Last {hours} hours\")\n",
    "            print(f\"Cutoff: {cutoff}\")\n",
    "            print(f\"---------------------------------------------------------------\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Get ALL documents - NO userId filter\n",
    "            print(f\"Querying heart_rate_data collection...\")\n",
    "            docs = self.db.collection('heart_rate_data')\\\n",
    "                .where('timestamp', '>=', cutoff)\\\n",
    "                .stream()\n",
    "            \n",
    "            docs_list = list(docs)\n",
    "            total_docs = len(docs_list)\n",
    "            \n",
    "            print(f\"Found {total_docs} documents\\n\")\n",
    "            \n",
    "            if total_docs == 0:\n",
    "                print(f\"NO DOCUMENTS FOUND in the last {hours} hours!\")\n",
    "                return {'processed': 0, 'skipped': 0, 'errors': 0, 'total': 0}\n",
    "            \n",
    "            processed = 0\n",
    "            skipped = 0\n",
    "            errors = 0\n",
    "            \n",
    "            for idx, doc in enumerate(docs_list, 1):\n",
    "                try:\n",
    "                    data = doc.to_dict()\n",
    "                    \n",
    "                    # Extract IBI data\n",
    "                    ibi_data = data.get('ibi', [])\n",
    "                    \n",
    "                    if not ibi_data:\n",
    "                        skipped += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate HRV\n",
    "                    hrv_metrics = self.calculate_hrv_from_ibi(ibi_data)\n",
    "                    \n",
    "                    if hrv_metrics is None:\n",
    "                        skipped += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Predict stress\n",
    "                    prediction = self.predict_stress(hrv_metrics)\n",
    "                    \n",
    "                    if prediction is None:\n",
    "                        skipped += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Save prediction - use default userId and email\n",
    "                    self.db.collection('stress_predictions2').add({\n",
    "                        'userId': self.default_user_id,\n",
    "                        'userEmail': self.default_email,\n",
    "                        'stress_probabilities': prediction['probabilities'],\n",
    "                        'stressLevel': prediction['stressLevel'],\n",
    "                        'stressLabel': prediction['stressLabel'],\n",
    "                        'confidence': prediction['confidence'],\n",
    "                        'prediction_timestamp': firestore.SERVER_TIMESTAMP,\n",
    "                        'source': 'batch_prediction',\n",
    "                        'source_doc_id': doc.id,\n",
    "                        'hrv_metrics': hrv_metrics\n",
    "                    })\n",
    "                    \n",
    "                    processed += 1\n",
    "                    \n",
    "                    # Progress indicator\n",
    "                    if verbose and processed % 50 == 0:\n",
    "                        print(f\"  Progress: {idx}/{total_docs} docs | {processed} processed | {skipped} skipped\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if verbose:\n",
    "                        print(f\"  Error on doc {doc.id}: {str(e)}\")\n",
    "                    errors += 1\n",
    "            \n",
    "            result = {\n",
    "                'processed': processed,\n",
    "                'skipped': skipped,\n",
    "                'errors': errors,\n",
    "                'total': total_docs\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n---------------------------------------------------------------\")\n",
    "            print(f\"BATCH PROCESSING COMPLETE\")\n",
    "            print(f\"---------------------------------------------------------------\")\n",
    "            print(f\"  Successfully processed: {processed}\")\n",
    "            print(f\"  Skipped (insufficient data): {skipped}\")\n",
    "            print(f\"  Errors: {errors}\")\n",
    "            print(f\"  Total documents checked: {total_docs}\")\n",
    "            print(f\"  Success rate: {(processed/total_docs*100):.1f}%\")\n",
    "            print(f\"---------------------------------------------------------------\\n\")\n",
    "            \n",
    "            if processed > 0:\n",
    "                print(f\"{processed} predictions saved to 'stress_predictions2' collection!\")\n",
    "            else:\n",
    "                print(f\"No predictions were saved. Check the skipped count above.\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nFATAL ERROR in batch processing\")\n",
    "            print(f\"   Error: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return {'error': str(e)}\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# DEBUGGING UTILITIES\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def quick_data_check(db, hours=24):\n",
    "    \"\"\"Quick check of what's in the database\"\"\"\n",
    "    print(f\"\\n---------------------------------------------------------------\")\n",
    "    print(f\"QUICK DATA CHECK\")\n",
    "    print(f\"---------------------------------------------------------------\\n\")\n",
    "    \n",
    "    cutoff = datetime.now() - timedelta(hours=hours)\n",
    "    \n",
    "    # Check heart_rate_data\n",
    "    print(f\"Checking heart_rate_data (last {hours} hours)...\")\n",
    "    docs = db.collection('heart_rate_data').where('timestamp', '>=', cutoff).stream()\n",
    "    docs_list = list(docs)\n",
    "    \n",
    "    print(f\"   Total documents: {len(docs_list)}\")\n",
    "    \n",
    "    if len(docs_list) > 0:\n",
    "        # Count documents with valid IBI data\n",
    "        valid_ibi = 0\n",
    "        total_ibi_values = 0\n",
    "        \n",
    "        for doc in docs_list:\n",
    "            data = doc.to_dict()\n",
    "            ibi = data.get('ibi', [])\n",
    "            if isinstance(ibi, list) and len(ibi) >= 2:\n",
    "                valid_ibi += 1\n",
    "                total_ibi_values += len(ibi)\n",
    "        \n",
    "        print(f\"   Documents with valid IBI (â‰¥2 values): {valid_ibi}\")\n",
    "        print(f\"   Total IBI values: {total_ibi_values}\")\n",
    "        print(f\"   Average IBI per doc: {total_ibi_values/len(docs_list):.1f}\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(f\"\\n   Sample document:\")\n",
    "        sample = docs_list[0].to_dict()\n",
    "        print(f\"      Fields: {list(sample.keys())}\")\n",
    "        print(f\"      IBI count: {len(sample.get('ibi', []))}\")\n",
    "        if sample.get('ibi'):\n",
    "            print(f\"      IBI sample: {sample['ibi'][:10]}\")\n",
    "    \n",
    "    # Check stress_predictions2\n",
    "    print(f\"\\nChecking stress_predictions2...\")\n",
    "    pred_docs = db.collection('stress_predictions2').stream()\n",
    "    pred_docs_list = list(pred_docs)\n",
    "    \n",
    "    print(f\"   Total predictions: {len(pred_docs_list)}\")\n",
    "    \n",
    "    if len(pred_docs_list) > 0:\n",
    "        recent = pred_docs_list[-1].to_dict()\n",
    "        print(f\"\\n   Most recent prediction:\")\n",
    "        print(f\"      Stress level: {recent.get('stressLabel', 'N/A')}\")\n",
    "        print(f\"      Confidence: {recent.get('confidence', 'N/A'):.1%}\")\n",
    "        print(f\"      Source: {recent.get('source', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\n---------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3acf4b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "\n",
    "import os\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, firestore\n",
    "from datetime import datetime, timedelta\n",
    "import joblib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba4de6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for Firebase credentials...\n",
      "cred path: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver2 - Copy\\HealMind_Ver3\\healmind-2025-firebase-adminsdk-fbsvc-12242dbda6.json\n",
      "connecting to firebase...\n",
      "firebase connected\n",
      "searching for model files...\n",
      "model found: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver2 - Copy\\HealMind_Ver3\\HRVModule\\XG_Calc_predict\\stress_model.pkl\n",
      "scaler found: d:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver2 - Copy\\HealMind_Ver3\\HRVModule\\XG_Calc_predict\\scaler.pkl\n",
      "loading model...\n",
      "model loaded\n",
      "loading scaler...\n",
      "scaler loaded\n"
     ]
    }
   ],
   "source": [
    "# find firebase credentials\n",
    "\n",
    "print(\"Looking for Firebase credentials...\")\n",
    "\n",
    "startPath = os.path.abspath('../../')\n",
    "credPath = None\n",
    "\n",
    "for root, dirs, files in os.walk(startPath):\n",
    "    if 'healmind-2025-firebase-adminsdk-fbsvc-12242dbda6.json' in files:\n",
    "        credPath = os.path.join(root, 'healmind-2025-firebase-adminsdk-fbsvc-12242dbda6.json')\n",
    "        break\n",
    "\n",
    "print(\"cred path:\", credPath)\n",
    "\n",
    "\n",
    "# connect firebase\n",
    "\n",
    "print(\"connecting to firebase...\")\n",
    "\n",
    "cred = credentials.Certificate(credPath)\n",
    "\n",
    "try:\n",
    "    firebase_admin.initialize_app(cred)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "db = firestore.client()\n",
    "print(\"firebase connected\")\n",
    "\n",
    "\n",
    "# search for model + scaler\n",
    "\n",
    "print(\"searching for model files...\")\n",
    "\n",
    "startPath = os.path.abspath('../../')   # reused\n",
    "modelPath = None\n",
    "scalerPath = None\n",
    "\n",
    "for root, dirs, files in os.walk(startPath):\n",
    "    if 'stress_model.pkl' in files:\n",
    "        modelPath = os.path.join(root, 'stress_model.pkl')\n",
    "        print(\"model found:\", modelPath)\n",
    "\n",
    "    if 'scaler.pkl' in files:\n",
    "        scalerPath = os.path.join(root, 'scaler.pkl')\n",
    "        print(\"scaler found:\", scalerPath)\n",
    "\n",
    "\n",
    "# load model stuff\n",
    "\n",
    "print(\"loading model...\")\n",
    "model = joblib.load(modelPath)\n",
    "print(\"model loaded\")\n",
    "\n",
    "print(\"loading scaler...\")\n",
    "scaler = joblib.load(scalerPath)\n",
    "print(\"scaler loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fe9aecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictor...\n",
      "Model and scaler loaded successfully\n",
      "Default user: test@email.com (ID: QOyZROlPzUf25tKPvv0FWnd3NZw2)\n",
      "predictor ready\n",
      "SETUP DONE\n",
      "running batch...\n",
      "\n",
      "BATCH PROCESSING - ALL DATA\n",
      "Time window: Last 1 hours\n",
      "Cutoff: 2026-01-20 03:00:36.478498\n",
      "\n",
      "Querying heart_rate_data collection...\n",
      "\n",
      "FATAL ERROR in batch processing\n",
      "   Error: Timeout of 300.0s exceeded, last exception: 429 Quota exceeded.\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 169, in error_remapped_callable\n",
      "    return _StreamingResponseIterator(\n",
      "  File \"d:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 91, in __init__\n",
      "    self._stored_first_result = next(self._wrapped)\n",
      "  File \"d:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\grpc\\_channel.py\", line 538, in __next__\n",
      "    return self._next()\n",
      "  File \"d:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\grpc\\_channel.py\", line 962, in _next\n",
      "    raise self\n",
      "grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\n",
      "\tstatus = StatusCode.RESOURCE_EXHAUSTED\n",
      "\tdetails = \"Quota exceeded.\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv6:%5B2404:6800:4001:818::200a%5D:443 {grpc_message:\"Quota exceeded.\", grpc_status:8}\"\n",
      ">\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 147, in retry_target\n",
      "    result = target()\n",
      "  File \"d:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\api_core\\timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "  File \"d:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 173, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 Quota exceeded.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\laiba\\AppData\\Local\\Temp\\ipykernel_28872\\3162972063.py\", line 99, in run_batch\n",
      "    docs_list = list(docs)\n",
      "  File \"d:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\cloud\\firestore_v1\\stream_generator.py\", line 58, in __next__\n",
      "    return self._generator.__next__()\n",
      "  File \"d:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\cloud\\firestore_v1\\query.py\", line 422, in _make_stream\n",
      "    response_iterator, expected_prefix = self._get_stream_iterator(\n",
      "  File \"d:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\cloud\\firestore_v1\\query.py\", line 267, in _get_stream_iterator\n",
      "    response_iterator = self._client._firestore_api.run_query(\n",
      "  File \"d:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\cloud\\firestore_v1\\services\\firestore\\client.py\", line 1643, in run_query\n",
      "    response = rpc(\n",
      "  File \"d:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "  File \"d:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 294, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "  File \"d:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 156, in retry_target\n",
      "    next_sleep = _retry_error_helper(\n",
      "  File \"d:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py\", line 229, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "google.api_core.exceptions.RetryError: Timeout of 300.0s exceeded, last exception: 429 Quota exceeded.\n"
     ]
    }
   ],
   "source": [
    "# predictor\n",
    "\n",
    "print(\"predictor...\")\n",
    "\n",
    "predictor = StressPredictor(\n",
    "    model_file=modelPath,\n",
    "    scaler_file=scalerPath,\n",
    "    db=db,\n",
    "    default_user_id='QOyZROlPzUf25tKPvv0FWnd3NZw2',\n",
    "    default_email='test@email.com'\n",
    ")\n",
    "\n",
    "print(\"predictor ready\")\n",
    "\n",
    "\n",
    "print(\"SETUP DONE\")\n",
    "\n",
    "\n",
    "#  run it\n",
    "\n",
    "print(\"running batch...\")\n",
    "\n",
    "result = predictor.run_batch(hours=1)\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041d4665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting continuous stress prediction...\n",
      "initial last_checked_time: 2026-01-19 18:35:05.347902+00:00\n",
      "\n",
      "checking for new heart rate data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_MultiThreadedRendezvous\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:169\u001b[0m, in \u001b[0;36m_wrap_stream_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     prefetch_first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callable_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_prefetch_first_result_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_StreamingResponseIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefetch_first_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefetch_first\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32md:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:91\u001b[0m, in \u001b[0;36m_StreamingResponseIterator.__init__\u001b[1;34m(self, wrapped, prefetch_first_result)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prefetch_first_result:\n\u001b[1;32m---> 91\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stored_first_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m# It is possible the wrapped method isn't an iterable (a grpc.Call\u001b[39;00m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;66;03m# for instance). If this happens don't store the first result.\u001b[39;00m\n",
      "File \u001b[1;32md:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\grpc\\_channel.py:538\u001b[0m, in \u001b[0;36m_Rendezvous.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\grpc\\_channel.py:962\u001b[0m, in \u001b[0;36m_MultiThreadedRendezvous._next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 962\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31m_MultiThreadedRendezvous\u001b[0m: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.RESOURCE_EXHAUSTED\n\tdetails = \"Quota exceeded.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv6:%5B2404:6800:4001:818::200a%5D:443 {grpc_message:\"Quota exceeded.\", grpc_status:8}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32md:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
      "File \u001b[1;32md:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:173\u001b[0m, in \u001b[0;36m_wrap_stream_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mResourceExhausted\u001b[0m: 429 Quota exceeded.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 27\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# query only NEW documents\u001b[39;00m\n\u001b[0;32m     20\u001b[0m docs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     21\u001b[0m     db\u001b[38;5;241m.\u001b[39mcollection(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheart_rate_data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m'\u001b[39m, last_checked_time)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;241m.\u001b[39morder_by(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;241m.\u001b[39mstream()\n\u001b[0;32m     25\u001b[0m )\n\u001b[1;32m---> 27\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew docs found:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(docs))\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(docs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\cloud\\firestore_v1\\stream_generator.py:58\u001b[0m, in \u001b[0;36mStreamGenerator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;66;03m# If explain_metrics is available, it would be returned.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mvalue:\n",
      "File \u001b[1;32md:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\cloud\\firestore_v1\\query.py:422\u001b[0m, in \u001b[0;36mQuery._make_stream\u001b[1;34m(self, transaction, retry, timeout, explain_options, read_time)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Internal method for stream(). Read the documents in the collection\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;124;03mthat match this query.\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03m    The results of query profiling, if received from the service.\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    420\u001b[0m metrics: ExplainMetrics \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 422\u001b[0m response_iterator, expected_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_stream_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransaction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplain_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    430\u001b[0m last_snapshot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32md:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\cloud\\firestore_v1\\query.py:267\u001b[0m, in \u001b[0;36mQuery._get_stream_iterator\u001b[1;34m(self, transaction, retry, timeout, explain_options, read_time)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper method for :meth:`stream`.\"\"\"\u001b[39;00m\n\u001b[0;32m    263\u001b[0m request, expected_prefix, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prep_stream(\n\u001b[0;32m    264\u001b[0m     transaction, retry, timeout, explain_options, read_time\n\u001b[0;32m    265\u001b[0m )\n\u001b[1;32m--> 267\u001b[0m response_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_firestore_api\u001b[38;5;241m.\u001b[39mrun_query(\n\u001b[0;32m    268\u001b[0m     request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[0;32m    269\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_rpc_metadata,\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    271\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response_iterator, expected_prefix\n",
      "File \u001b[1;32md:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\cloud\\firestore_v1\\services\\firestore\\client.py:1643\u001b[0m, in \u001b[0;36mFirestoreClient.run_query\u001b[1;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m   1640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m   1642\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m-> 1643\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1648\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1650\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m   1651\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32md:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    293\u001b[0m )\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:167\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m next_sleep \u001b[38;5;241m=\u001b[39m _retry_error_helper(\n\u001b[0;32m    157\u001b[0m     exc,\n\u001b[0;32m    158\u001b[0m     deadline,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    164\u001b[0m     timeout,\n\u001b[0;32m    165\u001b[0m )\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_sleep\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# continuous prediction loop (keeps running)\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"starting continuous stress prediction...\")\n",
    "\n",
    "# keep track of last processed timestamp\n",
    "from datetime import timezone\n",
    "\n",
    "last_checked_time = datetime.now(timezone.utc) - timedelta(minutes=5)\n",
    "print(\"initial last_checked_time:\", last_checked_time)\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    try:\n",
    "        print(\"\\nchecking for new heart rate data...\")\n",
    "\n",
    "        # query only NEW documents\n",
    "        docs = (\n",
    "            db.collection('heart_rate_data')\n",
    "            .where('timestamp', '>', last_checked_time)\n",
    "            .order_by('timestamp')\n",
    "            .stream()\n",
    "        )\n",
    "\n",
    "        docs = list(docs)\n",
    "        print(\"new docs found:\", len(docs))\n",
    "\n",
    "        if len(docs) == 0:\n",
    "            print(\"no new data, waiting...\")\n",
    "        else:\n",
    "            for doc in docs:\n",
    "                data = doc.to_dict()\n",
    "\n",
    "                # update last processed time\n",
    "                ts = data.get('timestamp')\n",
    "                if ts:\n",
    "                    last_checked_time = max(last_checked_time, ts)\n",
    "\n",
    "                # basic sanity check\n",
    "                if 'ibi' not in data or len(data.get('ibi', [])) < 2:\n",
    "                    print(f\"skipping {doc.id} (invalid IBI)\")\n",
    "                    continue\n",
    "\n",
    "                # run prediction (your existing logic)\n",
    "                print(f\"predicting stress for doc: {doc.id}\")\n",
    "\n",
    "                result = predictor.runBatch(\n",
    "                    doc_id=doc.id,\n",
    "                    data=data\n",
    "                )\n",
    "\n",
    "                if result:\n",
    "                    print(f\"stored prediction for {doc.id}\")\n",
    "                else:\n",
    "                    print(f\"prediction skipped for {doc.id}\")\n",
    "\n",
    "        # wait before next poll\n",
    "        time.sleep(30)   # adjust if needed\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"error occurred:\", e)\n",
    "        print(\"sleeping before retry...\")\n",
    "        time.sleep(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
