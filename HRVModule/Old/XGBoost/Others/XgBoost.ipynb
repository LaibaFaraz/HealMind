{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42fc9420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder exists: True\n",
      "Contents: ['com.samsung.health.hrv', 'com.samsung.shealth.stress']\n",
      "Stress folder exists: True\n",
      "HRV folder exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = r'D:\\laiba\\Desktop\\HRClaude\\jsons'\n",
    "print(f\"Folder exists: {os.path.exists(data_dir)}\")\n",
    "print(f\"Contents: {os.listdir(data_dir)}\")\n",
    "\n",
    "stress_path = os.path.join(data_dir, 'com.samsung.shealth.stress')\n",
    "print(f\"Stress folder exists: {os.path.exists(stress_path)}\")\n",
    "\n",
    "hrv_path = os.path.join(data_dir, 'com.samsung.health.hrv')\n",
    "print(f\"HRV folder exists: {os.path.exists(hrv_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f17d37c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found:\n",
      "  Stress files: 119\n",
      "  HRV files: 35\n",
      "Loaded stress_data: 5091 records\n",
      "Loaded hrv_data: 2807 records\n",
      "First stress record columns: dict_keys(['score', 'score_max', 'score_min', 'flag', 'level', 'start_time', 'end_time'])\n",
      "First stress record: {'score': 0, 'score_max': 0, 'score_min': 0, 'flag': 1, 'level': 1, 'start_time': 1767300300000, 'end_time': 1767300359000}\n",
      "First HRV record columns: dict_keys(['start_time', 'end_time', 'sdnn', 'rmssd'])\n",
      "First HRV record: {'start_time': 1767088810348, 'end_time': 1767089112018, 'sdnn': 44.36093, 'rmssd': 52.021973}\n"
     ]
    }
   ],
   "source": [
    "loader = SamsungDataLoader()\n",
    "stress_data, hrv_data, hr_data = loader.load_all_samsung_files(data_dir)\n",
    "\n",
    "print(f\"Loaded stress_data: {len(stress_data)} records\")\n",
    "print(f\"Loaded hrv_data: {len(hrv_data)} records\")\n",
    "\n",
    "if stress_data:\n",
    "    print(f\"First stress record columns: {stress_data[0].keys()}\")\n",
    "    print(f\"First stress record: {stress_data[0]}\")\n",
    "else:\n",
    "    print(\"ERROR: stress_data is EMPTY!\")\n",
    "\n",
    "if hrv_data:\n",
    "    print(f\"First HRV record columns: {hrv_data[0].keys()}\")\n",
    "    print(f\"First HRV record: {hrv_data[0]}\")\n",
    "else:\n",
    "    print(\"ERROR: hrv_data is EMPTY!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9caffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STRESS DETECTION MODEL TRAINING\n",
      "======================================================================\n",
      "\n",
      "Searching for WESAD directory...\n",
      "Found WESAD at: ./WESAD\n",
      "\n",
      "Loading Samsung Health files...\n",
      "Found:\n",
      "  Stress files: 119\n",
      "  HRV files: 35\n",
      "Loaded 2807 HRV records, 5091 stress records\n",
      "\n",
      "Processing Samsung Health data...\n",
      "Stress range: 0.0 - 100.0\n",
      "Categories: Low=1711, Moderate=1705, High=1675\n",
      "Merged samples: 855\n",
      "  Low: 419, Moderate: 229, High: 207\n",
      "\n",
      "Loading WESAD dataset...\n",
      "WESAD samples: 0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'stressCategory'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 445\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wesadDirectory \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(wesadDirectory):\n\u001b[0;32m    444\u001b[0m     wesadProcessor \u001b[38;5;241m=\u001b[39m WesadProcessor()\n\u001b[1;32m--> 445\u001b[0m     wesadDf \u001b[38;5;241m=\u001b[39m \u001b[43mwesadProcessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadWesad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwesadDirectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWESAD directory not available\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 153\u001b[0m, in \u001b[0;36mWesadProcessor.loadWesad\u001b[1;34m(self, wesadPath)\u001b[0m\n\u001b[0;32m    151\u001b[0m wesadDf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(allSamples)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWESAD samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(wesadDf)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 153\u001b[0m lowCount \u001b[38;5;241m=\u001b[39m (\u001b[43mwesadDf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstressCategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    154\u001b[0m moderateCount \u001b[38;5;241m=\u001b[39m (wesadDf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstressCategory\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    155\u001b[0m highCount \u001b[38;5;241m=\u001b[39m (wesadDf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstressCategory\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32md:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32md:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'stressCategory'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import glob\n",
    "import pickle\n",
    "from scipy.signal import find_peaks\n",
    "import os\n",
    "\n",
    "\n",
    "class SamsungProcessor:\n",
    "    \"\"\"handles samsung health data processing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "    \n",
    "    def loadAndProcess(self, hrvData, stressData):\n",
    "        print(\"\\nProcessing Samsung Health data...\")\n",
    "        \n",
    "        # convert to dataframes\n",
    "        hrvDf = pd.DataFrame(hrvData)\n",
    "        stressDf = pd.DataFrame(stressData)\n",
    "        \n",
    "        # fix timestamps\n",
    "        hrvDf['timestamp'] = pd.to_datetime(hrvDf['start_time'], unit='ms')\n",
    "        stressDf['timestamp'] = pd.to_datetime(stressDf['start_time'], unit='ms')\n",
    "        \n",
    "        # create 3 stress categories - found this on stackoverflow\n",
    "        stressDf['stressCategory'] = pd.qcut(stressDf['score'], q=3, labels=[0, 1, 2], duplicates='drop')\n",
    "        stressDf['stressCategory'] = stressDf['stressCategory'].astype(int)\n",
    "        \n",
    "        print(f\"Stress range: {stressDf['score'].min():.1f} - {stressDf['score'].max():.1f}\")\n",
    "        lowCount = (stressDf['stressCategory']==0).sum()\n",
    "        moderateCount = (stressDf['stressCategory']==1).sum()\n",
    "        highCount = (stressDf['stressCategory']==2).sum()\n",
    "        print(f\"Categories: Low={lowCount}, Moderate={moderateCount}, High={highCount}\")\n",
    "        \n",
    "        # merge the data\n",
    "        mergedData = pd.merge_asof(\n",
    "            hrvDf.sort_values('timestamp'),\n",
    "            stressDf.sort_values('timestamp'),\n",
    "            on='timestamp',\n",
    "            direction='nearest',\n",
    "            tolerance=pd.Timedelta('1hr')\n",
    "        )\n",
    "        \n",
    "        mergedData = mergedData.dropna(subset=['sdnn', 'rmssd', 'stressCategory'])\n",
    "        mergedData['dataSource'] = 'samsung'\n",
    "        \n",
    "        print(f\"Merged samples: {len(mergedData)}\")\n",
    "        lowCount = (mergedData['stressCategory']==0).sum()\n",
    "        moderateCount = (mergedData['stressCategory']==1).sum()\n",
    "        highCount = (mergedData['stressCategory']==2).sum()\n",
    "        print(f\"  Low: {lowCount}, Moderate: {moderateCount}, High: {highCount}\")\n",
    "        \n",
    "        self.data = mergedData\n",
    "        return mergedData\n",
    "\n",
    "\n",
    "class WesadProcessor:\n",
    "    \"\"\"processes wesad dataset\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "    \n",
    "    def computeHrv(self, ecgSignal):\n",
    "        # peak detection algorithm\n",
    "        peaks, _ = find_peaks(ecgSignal, distance=420, height=0)\n",
    "        rrIntervals = np.diff(peaks) / 700 * 1000\n",
    "        \n",
    "        # filter bad values\n",
    "        rrIntervals = rrIntervals[(rrIntervals > 300) & (rrIntervals < 2000)]\n",
    "        \n",
    "        if len(rrIntervals) < 10:\n",
    "            return None, None\n",
    "        \n",
    "        # calculate metrics\n",
    "        sdnnValue = np.std(rrIntervals)\n",
    "        rmssdValue = np.sqrt(np.mean(np.diff(rrIntervals)**2))\n",
    "        \n",
    "        return sdnnValue, rmssdValue\n",
    "    \n",
    "    def loadWesad(self, wesadPath):\n",
    "        print(\"\\nLoading WESAD dataset...\")\n",
    "        \n",
    "        allSamples = []\n",
    "        subjectList = ['S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', \n",
    "                      'S10', 'S11', 'S13', 'S14', 'S15', 'S16', 'S17']\n",
    "        \n",
    "        for subjId in subjectList:\n",
    "            pklFile = f\"{wesadPath}/{subjId}/{subjId}.pkl\"\n",
    "            \n",
    "            if not os.path.exists(pklFile):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                with open(pklFile, 'rb') as f:\n",
    "                    subjData = pickle.load(f, encoding='latin1')\n",
    "                \n",
    "                ecgData = subjData['signal']['chest']['ECG'].flatten()\n",
    "                labelData = subjData['label']\n",
    "                \n",
    "                # process windows - 60 seconds each\n",
    "                windowSize = 42000\n",
    "                stepSize = 21000\n",
    "                \n",
    "                for i in range(0, len(ecgData) - windowSize, stepSize):\n",
    "                    ecgWindow = ecgData[i:i+windowSize]\n",
    "                    labelWindow = labelData[i:i+windowSize]\n",
    "                    \n",
    "                    try:\n",
    "                        sdnn, rmssd = self.computeHrv(ecgWindow)\n",
    "                        \n",
    "                        if sdnn is None:\n",
    "                            continue\n",
    "                        \n",
    "                        # get majority label\n",
    "                        majorityLabel = np.bincount(labelWindow).argmax()\n",
    "                        \n",
    "                        # wesad labels: \n",
    "                        if majorityLabel == 1:\n",
    "                            stressClass = 0  # baseline = low\n",
    "                        elif majorityLabel == 2:\n",
    "                            stressClass = 2  # stress = high\n",
    "                        elif majorityLabel == 3:\n",
    "                            stressClass = 1  # amusement = moderate\n",
    "                        else:\n",
    "                            continue\n",
    "                        \n",
    "                        allSamples.append({\n",
    "                            'sdnn': sdnn,\n",
    "                            'rmssd': rmssd,\n",
    "                            'stressCategory': stressClass,\n",
    "                            'dataSource': 'wesad'\n",
    "                        })\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                print(f\"  Loaded {subjId}\")\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        wesadDf = pd.DataFrame(allSamples)\n",
    "        print(f\"WESAD samples: {len(wesadDf)}\")\n",
    "        lowCount = (wesadDf['stressCategory']==0).sum()\n",
    "        moderateCount = (wesadDf['stressCategory']==1).sum()\n",
    "        highCount = (wesadDf['stressCategory']==2).sum()\n",
    "        print(f\"Low: {lowCount}, Moderate: {moderateCount}, High: {highCount}\")\n",
    "        \n",
    "        self.data = wesadDf\n",
    "        return wesadDf\n",
    "\n",
    "\n",
    "class StressClassifier:\n",
    "    \"\"\"trains xgboost model for stress classification\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = None\n",
    "        self.useHybrid = False\n",
    "    \n",
    "    def trainHybridModel(self, samsungData, wesadData):\n",
    "        \"\"\"train using both wesad and samsung data\"\"\"\n",
    "        self.useHybrid = True\n",
    "        print(\"\\n=== Training Model ===\")\n",
    "        \n",
    "        # combine datasets\n",
    "        combinedData = pd.concat([wesadData, samsungData], ignore_index=True)\n",
    "        print(f\"Total samples: {len(combinedData)} (WESAD: {len(wesadData)}, Samsung: {len(samsungData)})\")\n",
    "        \n",
    "        # prepare features\n",
    "        allFeatures = combinedData[['sdnn', 'rmssd']].values\n",
    "        allLabels = combinedData['stressCategory'].values\n",
    "        sourceLabels = combinedData['dataSource'].values\n",
    "        \n",
    "        # split by data source\n",
    "        isSamsung = sourceLabels == 'samsung'\n",
    "        samsungFeatures = allFeatures[isSamsung]\n",
    "        samsungLabels = allLabels[isSamsung]\n",
    "        wesadFeatures = allFeatures[~isSamsung]\n",
    "        wesadLabels = allLabels[~isSamsung]\n",
    "        \n",
    "        wesadLow = (wesadLabels==0).sum()\n",
    "        wesadMod = (wesadLabels==1).sum()\n",
    "        wesadHigh = (wesadLabels==2).sum()\n",
    "        print(f\"WESAD distribution - Low: {wesadLow}, Moderate: {wesadMod}, High: {wesadHigh}\")\n",
    "        \n",
    "        samsungLow = (samsungLabels==0).sum()\n",
    "        samsungMod = (samsungLabels==1).sum()\n",
    "        samsungHigh = (samsungLabels==2).sum()\n",
    "        print(f\"Samsung distribution - Low: {samsungLow}, Moderate: {samsungMod}, High: {samsungHigh}\")\n",
    "        \n",
    "        # split wesad for validation\n",
    "        xTrain, xVal, yTrain, yVal = train_test_split(\n",
    "            wesadFeatures, wesadLabels, test_size=0.2, random_state=42, stratify=wesadLabels\n",
    "        )\n",
    "        \n",
    "        # scale the features\n",
    "        xTrain = self.scaler.fit_transform(xTrain)\n",
    "        xVal = self.scaler.transform(xVal)\n",
    "        \n",
    "        # train on wesad\n",
    "        print(\"\\nTraining on WESAD...\")\n",
    "        self.model = xgb.XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            eval_metric='mlogloss',\n",
    "            num_class=3\n",
    "        )\n",
    "        \n",
    "        self.model.fit(xTrain, yTrain, verbose=False)\n",
    "        \n",
    "        # check validation accuracy\n",
    "        predVal = self.model.predict(xVal)\n",
    "        valAcc = (predVal == yVal).mean()\n",
    "        print(f\"Validation accuracy: {valAcc:.3f}\")\n",
    "        \n",
    "        #finetune on samsung \n",
    "        print(\"\\nFine-tuning on Samsung...\")\n",
    "        samsungScaled = self.scaler.transform(samsungFeatures)\n",
    "        \n",
    "        self.model.learning_rate = 0.05\n",
    "        self.model.n_estimators = 50\n",
    "        existingModel = self.model.get_booster()\n",
    "        self.model.fit(samsungScaled, samsungLabels, verbose=False, xgb_model=existingModel)\n",
    "        \n",
    "        # evaluate final model\n",
    "        predSamsung = self.model.predict(samsungScaled)\n",
    "        probaSamsung = self.model.predict_proba(samsungScaled)\n",
    "        \n",
    "        print(\"\\n--- Final Results (Samsung) ---\")\n",
    "        uniqueClasses = np.unique(samsungLabels)\n",
    "        \n",
    "        if len(uniqueClasses) >= 2:\n",
    "            classNames = ['Low', 'Moderate', 'High']\n",
    "            targetNames = [classNames[int(i)] for i in uniqueClasses]\n",
    "            print(classification_report(samsungLabels, predSamsung, target_names=targetNames, labels=uniqueClasses))\n",
    "            print(f\"\\nConfusion Matrix:\\n{confusion_matrix(samsungLabels, predSamsung)}\")\n",
    "            \n",
    "            try:\n",
    "                aucScore = roc_auc_score(samsungLabels, probaSamsung, multi_class='ovr', average='weighted')\n",
    "                print(f\"AUC: {aucScore:.3f}\")\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            print(f\"Only {len(uniqueClasses)} class found\")\n",
    "            print(f\"Accuracy: {(predSamsung==samsungLabels).mean():.3f}\")\n",
    "        \n",
    "        print(f\"\\nFeature importance: SDNN={self.model.feature_importances_[0]:.3f}, RMSSD={self.model.feature_importances_[1]:.3f}\")\n",
    "    \n",
    "    def trainSamsungOnly(self, samsungData):\n",
    "        \"\"\"train using only samsung data\"\"\"\n",
    "        self.useHybrid = False\n",
    "        print(\"\\n=== Samsung-Only Training ===\")\n",
    "        \n",
    "        features = samsungData[['sdnn', 'rmssd']].values\n",
    "        labels = samsungData['stressCategory'].values\n",
    "        \n",
    "        lowCount = (labels==0).sum()\n",
    "        moderateCount = (labels==1).sum()\n",
    "        highCount = (labels==2).sum()\n",
    "        print(f\"Distribution - Low: {lowCount}, Moderate: {moderateCount}, High: {highCount}\")\n",
    "        \n",
    "        uniqueClasses = np.unique(labels)\n",
    "        if len(uniqueClasses) < 2:\n",
    "            print(\"ERROR: Need at least 2 classes\")\n",
    "            return\n",
    "        \n",
    "        # split data\n",
    "        xTrain, xTest, yTrain, yTest = train_test_split(\n",
    "            features, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "        )\n",
    "        \n",
    "        # scale features\n",
    "        xTrain = self.scaler.fit_transform(xTrain)\n",
    "        xTest = self.scaler.transform(xTest)\n",
    "        \n",
    "        # train model\n",
    "        self.model = xgb.XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            eval_metric='mlogloss',\n",
    "            num_class=3\n",
    "        )\n",
    "        \n",
    "        self.model.fit(xTrain, yTrain, verbose=False)\n",
    "        \n",
    "        # evaluate\n",
    "        pred = self.model.predict(xTest)\n",
    "        proba = self.model.predict_proba(xTest)\n",
    "        \n",
    "        print(\"\\n--- Results ---\")\n",
    "        testClasses = np.unique(yTest)\n",
    "        \n",
    "        if len(testClasses) >= 2:\n",
    "            classNames = ['Low', 'Moderate', 'High']\n",
    "            targetNames = [classNames[int(i)] for i in testClasses]\n",
    "            print(classification_report(yTest, pred, target_names=targetNames, labels=testClasses))\n",
    "            print(f\"\\nConfusion Matrix:\\n{confusion_matrix(yTest, pred)}\")\n",
    "            \n",
    "            try:\n",
    "                aucScore = roc_auc_score(yTest, proba, multi_class='ovr', average='weighted')\n",
    "                print(f\"AUC: {aucScore:.3f}\")\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            print(f\"Only {len(testClasses)} class in test\")\n",
    "            print(f\"Accuracy: {(pred==yTest).mean():.3f}\")\n",
    "        \n",
    "        print(f\"\\nFeature importance: SDNN={self.model.feature_importances_[0]:.3f}, RMSSD={self.model.feature_importances_[1]:.3f}\")\n",
    "    \n",
    "    def saveModel(self):\n",
    "        \"\"\"save model and scaler to disk\"\"\"\n",
    "        if self.useHybrid:\n",
    "            modelPath = 'stressModelHybrid.pkl'\n",
    "            scalerPath = 'scalerHybrid.pkl'\n",
    "        else:\n",
    "            modelPath = 'stressModelSamsung.pkl'\n",
    "            scalerPath = 'scalerSamsung.pkl'\n",
    "        \n",
    "        joblib.dump(self.model, modelPath)\n",
    "        joblib.dump(self.scaler, scalerPath)\n",
    "        \n",
    "        print(f\"\\nSaved model: {modelPath}\")\n",
    "        print(f\"Saved scaler: {scalerPath}\")\n",
    "\n",
    "\n",
    "# main execution\n",
    "if __name__ == '__main__':\n",
    "    print(\"=\"*70)\n",
    "    print(\"STRESS DETECTION MODEL TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # paths\n",
    "    dataDirectory = r'D:\\laiba\\Desktop\\HRClaude\\jsons'\n",
    "    \n",
    "    # search for WESAD directory\n",
    "    def findWesadDirectory():\n",
    "        print(\"\\nSearching for WESAD directory...\")\n",
    "        \n",
    "        # check common locations first\n",
    "        possiblePaths = [\n",
    "            r'D:\\laiba\\Desktop\\HRClaude\\WESAD',\n",
    "            r'D:\\laiba\\Desktop\\WESAD',\n",
    "            './WESAD',\n",
    "            '../WESAD',\n",
    "            '../../WESAD',\n",
    "            os.path.join(os.getcwd(), 'WESAD'),\n",
    "        ]\n",
    "        \n",
    "        for path in possiblePaths:\n",
    "            if os.path.exists(path):\n",
    "                print(f\"Found WESAD at: {path}\")\n",
    "                return path\n",
    "        \n",
    "        # if not found, search from desktop\n",
    "        try:\n",
    "            desktop = os.path.join(os.path.expanduser('~'), 'Desktop')\n",
    "            if os.path.exists(desktop):\n",
    "                for root, dirs, files in os.walk(desktop):\n",
    "                    if 'WESAD' in dirs:\n",
    "                        wesadPath = os.path.join(root, 'WESAD')\n",
    "                        print(f\"Found WESAD at: {wesadPath}\")\n",
    "                        return wesadPath\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(\"WESAD directory not found\")\n",
    "        return None\n",
    "    \n",
    "    wesadDirectory = findWesadDirectory()\n",
    "    \n",
    "    # load samsung files\n",
    "    print(\"\\nLoading Samsung Health files...\")\n",
    "    \n",
    "    def load_all_samsung_files(data_directory):\n",
    "        # Search recursively for HRV files\n",
    "        hrv_files = glob.glob(data_directory + '/com.samsung.health.hrv/**/*.json', recursive=True)\n",
    "        \n",
    "        # Search recursively for stress files (shealth NOT health)\n",
    "        stress_files = glob.glob(data_directory + '/com.samsung.shealth.stress/**/*.json', recursive=True)\n",
    "        \n",
    "        hr_files = []\n",
    "        \n",
    "        print(f\"Found:\")\n",
    "        print(f\"  Stress files: {len(stress_files)}\")\n",
    "        print(f\"  HRV files: {len(hrv_files)}\")\n",
    "        \n",
    "        stress_data = []\n",
    "        for file in stress_files:\n",
    "            try:\n",
    "                with open(file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    if isinstance(data, list):\n",
    "                        stress_data.extend(data)\n",
    "                    else:\n",
    "                        stress_data.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "        \n",
    "        hrv_data = []\n",
    "        for file in hrv_files:\n",
    "            if 'stress' in file.lower() or 'heart_rate' in file.lower():\n",
    "                continue\n",
    "            try:\n",
    "                with open(file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    if isinstance(data, list):\n",
    "                        hrv_data.extend(data)\n",
    "                    else:\n",
    "                        hrv_data.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "        \n",
    "        return stress_data, hrv_data, hr_files\n",
    "    \n",
    "    stressRecords, hrvRecords, hrFiles = load_all_samsung_files(dataDirectory)\n",
    "    \n",
    "    print(f\"Loaded {len(hrvRecords)} HRV records, {len(stressRecords)} stress records\")\n",
    "    \n",
    "    # process samsung data\n",
    "    samsungProcessor = SamsungProcessor()\n",
    "    samsungDf = samsungProcessor.loadAndProcess(hrvRecords, stressRecords)\n",
    "    \n",
    "    # try to load wesad\n",
    "    wesadDf = None\n",
    "    if wesadDirectory and os.path.exists(wesadDirectory):\n",
    "        wesadProcessor = WesadProcessor()\n",
    "        wesadDf = wesadProcessor.loadWesad(wesadDirectory)\n",
    "    else:\n",
    "        print(f\"\\nWESAD directory not available\")\n",
    "    \n",
    "    # train the model\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    classifier = StressClassifier()\n",
    "    \n",
    "    if wesadDf is not None and len(wesadDf) > 0:\n",
    "        classifier.trainHybridModel(samsungDf, wesadDf)\n",
    "    else:\n",
    "        classifier.trainSamsungOnly(samsungDf)\n",
    "    \n",
    "    classifier.saveModel()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DONE!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "452d6db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samsung Health Stress Classification\n",
      "============================================================\n",
      "Found:\n",
      "  Stress files: 119\n",
      "  HRV files: 35\n",
      "\n",
      "Processing data...\n",
      "Stress data:\n",
      "  Samples: 5091\n",
      "  Score range: 0.0 - 100.0\n",
      "  High stress: 2512\n",
      "  Low stress: 2579\n",
      "HRV data:\n",
      "  Samples: 2807\n",
      "  SDNN range: 30.8 - 174.1\n",
      "  RMSSD range: 25.6 - 133.1\n",
      "\n",
      "Merging...\n",
      "Merged data:\n",
      "  Matched samples: 855\n",
      "  Time span: 2025-12-26 06:48:02.825000 to 2026-01-02 00:36:18.909000\n",
      "  Duration: 6 days\n",
      "\n",
      "Training...\n",
      "\n",
      "MODEL EVALUATION\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       0.81      0.89      0.85       117\n",
      "    Moderate       0.69      0.54      0.60        54\n",
      "\n",
      "    accuracy                           0.78       171\n",
      "   macro avg       0.75      0.71      0.72       171\n",
      "weighted avg       0.77      0.78      0.77       171\n",
      "\n",
      "Confusion Matrix:\n",
      "[[104  13]\n",
      " [ 25  29]]\n",
      "ROC-AUC Score: 0.859\n",
      "\n",
      "Feature Importance:\n",
      "  sdnn: 0.4425\n",
      "  rmssd: 0.5575\n",
      "\n",
      "ROC curve saved\n",
      "\n",
      "Model saved to stress_model.pkl\n",
      "Scaler saved to scaler.pkl\n",
      "\n",
      "TRAINING COMPLETE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\laiba\\anaconda3\\envs\\TF\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2687: UserWarning: labels size, 2, does not match size of target_names, 3\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "class SamsungDataLoader:\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_all_samsung_files(data_directory):\n",
    "        # Search recursively for HRV files\n",
    "        hrv_files = glob.glob(data_directory + '/com.samsung.health.hrv/**/*.json', recursive=True)\n",
    "        \n",
    "        # Search recursively for stress files (shealth NOT health)\n",
    "        stress_files = glob.glob(data_directory + '/com.samsung.shealth.stress/**/*.json', recursive=True)\n",
    "        \n",
    "        hr_files = []\n",
    "        \n",
    "        print(f\"Found:\")\n",
    "        print(f\"  Stress files: {len(stress_files)}\")\n",
    "        print(f\"  HRV files: {len(hrv_files)}\")\n",
    "        \n",
    "        stress_data = []\n",
    "        for file in stress_files:\n",
    "            try:\n",
    "                with open(file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    if isinstance(data, list):\n",
    "                        stress_data.extend(data)\n",
    "                    else:\n",
    "                        stress_data.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "        \n",
    "        hrv_data = []\n",
    "        for file in hrv_files:\n",
    "            if 'stress' in file.lower() or 'heart_rate' in file.lower():\n",
    "                continue\n",
    "            try:\n",
    "                with open(file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    if isinstance(data, list):\n",
    "                        hrv_data.extend(data)\n",
    "                    else:\n",
    "                        hrv_data.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "        \n",
    "        return stress_data, hrv_data, hr_files\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_stress_data(stress_data):\n",
    "        df = pd.DataFrame(stress_data)\n",
    "        df['timestamp'] = pd.to_datetime(df['start_time'], unit='ms')\n",
    "        \n",
    "        df['stress_normalized'] = df['score'] / df['score'].max()\n",
    "        median = df['score'].median()\n",
    "        df['stress_binary'] = (df['score'] > median).astype(int)\n",
    "        \n",
    "        print(f\"Stress data:\")\n",
    "        print(f\"  Samples: {len(df)}\")\n",
    "        print(f\"  Score range: {df['score'].min():.1f} - {df['score'].max():.1f}\")\n",
    "        print(f\"  High stress: {df['stress_binary'].sum()}\")\n",
    "        print(f\"  Low stress: {(df['stress_binary'] == 0).sum()}\")\n",
    "        \n",
    "        return df[['timestamp', 'score', 'stress_binary', 'stress_normalized']]\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_hrv_data(hrv_data):\n",
    "        df = pd.DataFrame(hrv_data)\n",
    "        df['timestamp'] = pd.to_datetime(df['start_time'], unit='ms')\n",
    "        \n",
    "        if 'sdnn' not in df.columns:\n",
    "            df['sdnn'] = np.nan\n",
    "        if 'rmssd' not in df.columns:\n",
    "            df['rmssd'] = np.nan\n",
    "        \n",
    "        print(f\"HRV data:\")\n",
    "        print(f\"  Samples: {len(df)}\")\n",
    "        print(f\"  SDNN range: {df['sdnn'].min():.1f} - {df['sdnn'].max():.1f}\")\n",
    "        print(f\"  RMSSD range: {df['rmssd'].min():.1f} - {df['rmssd'].max():.1f}\")\n",
    "        \n",
    "        return df[['timestamp', 'sdnn', 'rmssd']]\n",
    "    \n",
    "    @staticmethod\n",
    "    def merge_stress_and_hrv(df_stress, df_hrv):\n",
    "        df_stress['timestamp_rounded'] = df_stress['timestamp'].dt.round('1min')\n",
    "        df_hrv['timestamp_rounded'] = df_hrv['timestamp'].dt.round('1min')\n",
    "        \n",
    "        merged = pd.merge_asof(\n",
    "            df_hrv.sort_values('timestamp'),\n",
    "            df_stress.sort_values('timestamp'),\n",
    "            on='timestamp',\n",
    "            direction='nearest',\n",
    "            tolerance=pd.Timedelta('1hr')\n",
    "        )\n",
    "        \n",
    "        merged = merged.dropna(subset=['stress_binary'])\n",
    "        merged = merged.dropna(subset=['sdnn', 'rmssd'])\n",
    "        \n",
    "        print(f\"Merged data:\")\n",
    "        print(f\"  Matched samples: {len(merged)}\")\n",
    "        print(f\"  Time span: {merged['timestamp'].min()} to {merged['timestamp'].max()}\")\n",
    "        print(f\"  Duration: {(merged['timestamp'].max() - merged['timestamp'].min()).days} days\")\n",
    "        \n",
    "        return merged[['timestamp', 'sdnn', 'rmssd', 'stress_binary', 'score']]\n",
    "\n",
    "\n",
    "class SamsungStressClassifier:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = None\n",
    "        self.feature_names = ['sdnn', 'rmssd']\n",
    "    \n",
    "    def train(self, X, y, test_size=0.2):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        self.model = xgb.XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "        \n",
    "        self.model.fit(X_train_scaled, y_train, verbose=False)\n",
    "        \n",
    "        y_pred = self.model.predict(X_test_scaled)\n",
    "        y_pred_proba = self.model.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        print(\"\\nMODEL EVALUATION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        unique_classes = np.unique(y_test)\n",
    "        if len(unique_classes) > 1:\n",
    "            target_names = ['Low', 'Moderate', 'High']\n",
    "            print(classification_report(y_test, y_pred, target_names=target_names, labels=unique_classes))\n",
    "            print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "            print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba, multi_class='ovr'):.3f}\")\n",
    "        else:\n",
    "            print(f\"Warning: Only 1 class in test set\")\n",
    "            print(f\"Accuracy: {(y_pred == y_test).mean():.3f}\")\n",
    "        \n",
    "        print(\"\\nFeature Importance:\")\n",
    "        for feat, imp in zip(self.feature_names, self.model.feature_importances_):\n",
    "            print(f\"  {feat}: {imp:.4f}\")\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, label=f'ROC Curve')\n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend()\n",
    "        plt.savefig('roc_curve.png', dpi=150)\n",
    "        plt.close()\n",
    "        print(\"\\nROC curve saved\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def predict(self, X_new):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained!\")\n",
    "        X_scaled = self.scaler.transform(X_new)\n",
    "        predictions = self.model.predict(X_scaled)\n",
    "        probabilities = self.model.predict_proba(X_scaled)[:, 1]\n",
    "        return predictions, probabilities\n",
    "    \n",
    "    def save_model(self, model_path='stress_model.pkl', scaler_path='scaler.pkl'):\n",
    "        joblib.dump(self.model, model_path)\n",
    "        joblib.dump(self.scaler, scaler_path)\n",
    "        print(f\"\\nModel saved to {model_path}\")\n",
    "        print(f\"Scaler saved to {scaler_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Samsung Health Stress Classification\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    loader = SamsungDataLoader()\n",
    "    data_dir = r'D:\\laiba\\Desktop\\HRClaude\\jsons'    \n",
    "    stress_data, hrv_data, hr_data = loader.load_all_samsung_files(data_dir)\n",
    "    \n",
    "    \n",
    "    print(\"\\nProcessing data...\")\n",
    "    df_stress = loader.process_stress_data(stress_data)\n",
    "    df_hrv = loader.process_hrv_data(hrv_data)\n",
    "    \n",
    "    print(\"\\nMerging...\")\n",
    "    df = loader.merge_stress_and_hrv(df_stress, df_hrv)\n",
    "    \n",
    "    print(\"\\nTraining...\")\n",
    "    X = df[['sdnn', 'rmssd']].values\n",
    "    y = df['stress_binary'].values\n",
    "    \n",
    "    classifier = SamsungStressClassifier()\n",
    "    classifier.train(X, y)\n",
    "    classifier.save_model()\n",
    "    \n",
    "    print(\"\\nTRAINING COMPLETE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
