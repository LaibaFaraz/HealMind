{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ebf9669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in d:\\laiba\\desktop\\usm\\cat304w drafts\\working\\healmind_ver2 - copy\\healmind_ver3\\.venv\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: numpy<3,>=1.25.2 in d:\\laiba\\desktop\\usm\\cat304w drafts\\working\\healmind_ver2 - copy\\healmind_ver3\\.venv\\lib\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.11.4 in d:\\laiba\\desktop\\usm\\cat304w drafts\\working\\healmind_ver2 - copy\\healmind_ver3\\.venv\\lib\\site-packages (from imbalanced-learn) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in d:\\laiba\\desktop\\usm\\cat304w drafts\\working\\healmind_ver2 - copy\\healmind_ver3\\.venv\\lib\\site-packages (from imbalanced-learn) (1.8.0)\n",
      "Requirement already satisfied: sklearn-compat<0.2,>=0.1.5 in d:\\laiba\\desktop\\usm\\cat304w drafts\\working\\healmind_ver2 - copy\\healmind_ver3\\.venv\\lib\\site-packages (from imbalanced-learn) (0.1.5)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in d:\\laiba\\desktop\\usm\\cat304w drafts\\working\\healmind_ver2 - copy\\healmind_ver3\\.venv\\lib\\site-packages (from imbalanced-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in d:\\laiba\\desktop\\usm\\cat304w drafts\\working\\healmind_ver2 - copy\\healmind_ver3\\.venv\\lib\\site-packages (from imbalanced-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc151726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution:\n",
      "0.0    1400\n",
      "2.0     539\n",
      "1.0     415\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Balanced class distribution:\n",
      "0.0    1400\n",
      "2.0    1400\n",
      "1.0    1400\n",
      "Name: count, dtype: int64\n",
      "\n",
      "EVALUATION\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       0.50      0.44      0.47       280\n",
      "    Moderate       0.53      0.60      0.57       280\n",
      "        High       0.55      0.55      0.55       280\n",
      "\n",
      "    accuracy                           0.53       840\n",
      "   macro avg       0.53      0.53      0.53       840\n",
      "weighted avg       0.53      0.53      0.53       840\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[123  82  75]\n",
      " [ 62 169  49]\n",
      " [ 61  65 154]]\n",
      "\n",
      "ROC-AUC: 0.722\n",
      "\n",
      "Feature importance:\n",
      "  sdnn: 0.463\n",
      "  rmssd: 0.537\n",
      "\n",
      "Saved confusion.png\n",
      "\n",
      "Saved stress_model.pkl and scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# After loading your data (assuming you have X and y from your training script)\n",
    "print(\"Original class distribution:\")\n",
    "print(pd.Series(y).value_counts())\n",
    "\n",
    "# Option 1: Use SMOTE to balance classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "print(\"\\nBalanced class distribution:\")\n",
    "print(pd.Series(y_balanced).value_counts())\n",
    "\n",
    "# Now retrain your classifier with balanced data\n",
    "clf = StressClassifier()  # Your existing classifier class\n",
    "clf.train(X_balanced, y_balanced)\n",
    "clf.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d15aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stress Classification - 3 Classes\n",
      "==================================================\n",
      "\n",
      "Loading Samsung data...\n",
      "Found:\n",
      "  Stress files: 119\n",
      "  HRV files: 35\n",
      "Stress data:\n",
      "  Samples: 5091\n",
      "  Score range: 0.0 - 100.0\n",
      "  Low stress: 1711\n",
      "  Moderate stress: 1677\n",
      "  High stress: 1703\n",
      "HRV data:\n",
      "  Samples: 2807\n",
      "  SDNN range: 30.8 - 174.1\n",
      "  RMSSD range: 25.6 - 133.1\n",
      "Merged data:\n",
      "  Matched samples: 855\n",
      "  Time span: 2025-12-26 06:48:02.825000 to 2026-01-02 00:36:18.909000\n",
      "  Duration: 6 days\n",
      "Added 855 Samsung samples\n",
      "\n",
      "Loading WESAD data...\n",
      "\n",
      "Found 15 WESAD subjects\n",
      "Processing S10...\n",
      "Processing S11...\n",
      "Processing S13...\n",
      "Processing S14...\n",
      "Processing S15...\n",
      "Processing S16...\n",
      "Processing S17...\n",
      "Processing S2...\n",
      "Processing S3...\n",
      "Processing S4...\n",
      "Processing S5...\n",
      "Processing S6...\n",
      "Processing S7...\n",
      "Processing S8...\n",
      "Processing S9...\n",
      "\n",
      "WESAD data:\n",
      "  Total samples: 1499\n",
      "  Low: 981\n",
      "  Moderate: 186\n",
      "  High: 332\n",
      "  SDNN range: 10.4 - 256.0\n",
      "  RMSSD range: 3.6 - 271.4\n",
      "\n",
      "Combining datasets...\n",
      "\n",
      "Total samples: 2354\n",
      "\n",
      "Samsung:\n",
      "  Samples: 855\n",
      "  Low: 419\n",
      "  Moderate: 229\n",
      "  High: 207\n",
      "\n",
      "WESAD:\n",
      "  Samples: 1499\n",
      "  Low: 981\n",
      "  Moderate: 186\n",
      "  High: 332\n",
      "\n",
      "Training model...\n",
      "\n",
      "EVALUATION\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       0.64      0.92      0.76       280\n",
      "    Moderate       0.38      0.11      0.17        83\n",
      "        High       0.51      0.22      0.31       108\n",
      "\n",
      "    accuracy                           0.62       471\n",
      "   macro avg       0.51      0.42      0.41       471\n",
      "weighted avg       0.57      0.62      0.55       471\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[257  10  13]\n",
      " [ 64   9  10]\n",
      " [ 79   5  24]]\n",
      "\n",
      "ROC-AUC: 0.657\n",
      "\n",
      "Feature importance:\n",
      "  sdnn: 0.464\n",
      "  rmssd: 0.536\n",
      "\n",
      "Saved confusion.png\n",
      "\n",
      "Saved stress_model.pkl and scaler.pkl\n",
      "\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pickle\n",
    "from scipy import signal\n",
    "\n",
    "class SamsungDataLoader:\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_all_samsung_files(data_directory):\n",
    "        # Search recursively for HRV files\n",
    "        hrv_files = glob.glob(data_directory + '/com.samsung.health.hrv/**/*.json', recursive=True)\n",
    "        \n",
    "        # Search recursively for stress files (shealth NOT health)\n",
    "        stress_files = glob.glob(data_directory + '/com.samsung.shealth.stress/**/*.json', recursive=True)\n",
    "        \n",
    "        hr_files = []\n",
    "        \n",
    "        print(f\"Found:\")\n",
    "        print(f\"  Stress files: {len(stress_files)}\")\n",
    "        print(f\"  HRV files: {len(hrv_files)}\")\n",
    "        \n",
    "        stress_data = []\n",
    "        for file in stress_files:\n",
    "            try:\n",
    "                with open(file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    if isinstance(data, list):\n",
    "                        stress_data.extend(data)\n",
    "                    else:\n",
    "                        stress_data.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "        \n",
    "        hrv_data = []\n",
    "        for file in hrv_files:\n",
    "            if 'stress' in file.lower() or 'heart_rate' in file.lower():\n",
    "                continue\n",
    "            try:\n",
    "                with open(file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    if isinstance(data, list):\n",
    "                        hrv_data.extend(data)\n",
    "                    else:\n",
    "                        hrv_data.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "        \n",
    "        return stress_data, hrv_data, hr_files\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_stress_data(stress_data):\n",
    "        df = pd.DataFrame(stress_data)\n",
    "        df['timestamp'] = pd.to_datetime(df['start_time'], unit='ms')\n",
    "        \n",
    "        df['stress_normalized'] = df['score'] / df['score'].max()\n",
    "        \n",
    "        # split into 3 classes\n",
    "        low_thresh = df['score'].quantile(0.33)\n",
    "        high_thresh = df['score'].quantile(0.66)\n",
    "        \n",
    "        df['stress_class'] = 1  # default moderate\n",
    "        df.loc[df['score'] <= low_thresh, 'stress_class'] = 0  # low\n",
    "        df.loc[df['score'] > high_thresh, 'stress_class'] = 2  # high\n",
    "        \n",
    "        print(f\"Stress data:\")\n",
    "        print(f\"  Samples: {len(df)}\")\n",
    "        print(f\"  Score range: {df['score'].min():.1f} - {df['score'].max():.1f}\")\n",
    "        print(f\"  Low stress: {(df['stress_class'] == 0).sum()}\")\n",
    "        print(f\"  Moderate stress: {(df['stress_class'] == 1).sum()}\")\n",
    "        print(f\"  High stress: {(df['stress_class'] == 2).sum()}\")\n",
    "        \n",
    "        return df[['timestamp', 'score', 'stress_class', 'stress_normalized']]\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_hrv_data(hrv_data):\n",
    "        df = pd.DataFrame(hrv_data)\n",
    "        df['timestamp'] = pd.to_datetime(df['start_time'], unit='ms')\n",
    "        \n",
    "        if 'sdnn' not in df.columns:\n",
    "            df['sdnn'] = np.nan\n",
    "        if 'rmssd' not in df.columns:\n",
    "            df['rmssd'] = np.nan\n",
    "        \n",
    "        print(f\"HRV data:\")\n",
    "        print(f\"  Samples: {len(df)}\")\n",
    "        print(f\"  SDNN range: {df['sdnn'].min():.1f} - {df['sdnn'].max():.1f}\")\n",
    "        print(f\"  RMSSD range: {df['rmssd'].min():.1f} - {df['rmssd'].max():.1f}\")\n",
    "        \n",
    "        return df[['timestamp', 'sdnn', 'rmssd']]\n",
    "    \n",
    "    @staticmethod\n",
    "    def merge_stress_and_hrv(df_stress, df_hrv):\n",
    "        df_stress['timestamp_rounded'] = df_stress['timestamp'].dt.round('1min')\n",
    "        df_hrv['timestamp_rounded'] = df_hrv['timestamp'].dt.round('1min')\n",
    "        \n",
    "        merged = pd.merge_asof(\n",
    "            df_hrv.sort_values('timestamp'),\n",
    "            df_stress.sort_values('timestamp'),\n",
    "            on='timestamp',\n",
    "            direction='nearest',\n",
    "            tolerance=pd.Timedelta('1hr')\n",
    "        )\n",
    "        \n",
    "        merged = merged.dropna(subset=['stress_class'])\n",
    "        merged = merged.dropna(subset=['sdnn', 'rmssd'])\n",
    "        \n",
    "        print(f\"Merged data:\")\n",
    "        print(f\"  Matched samples: {len(merged)}\")\n",
    "        print(f\"  Time span: {merged['timestamp'].min()} to {merged['timestamp'].max()}\")\n",
    "        print(f\"  Duration: {(merged['timestamp'].max() - merged['timestamp'].min()).days} days\")\n",
    "        \n",
    "        return merged[['timestamp', 'sdnn', 'rmssd', 'stress_class', 'score']]\n",
    "\n",
    "\n",
    "class WESADDataLoader:\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_hrv(rr_intervals):\n",
    "        \"\"\"Calculate HRV stuff\"\"\"\n",
    "        if len(rr_intervals) < 2:\n",
    "            return None, None\n",
    "        \n",
    "        # clean the data a bit\n",
    "        rr_intervals = rr_intervals[(rr_intervals > 300) & (rr_intervals < 2000)]\n",
    "        \n",
    "        if len(rr_intervals) < 2:\n",
    "            return None, None\n",
    "        \n",
    "        sdnn = np.std(rr_intervals)\n",
    "        \n",
    "        diff_rr = np.diff(rr_intervals)\n",
    "        rmssd = np.sqrt(np.mean(diff_rr ** 2))\n",
    "        \n",
    "        return sdnn, rmssd\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_rr_from_ecg(ecg_signal, fs=700):\n",
    "        \"\"\"Extract RR intervals\"\"\"\n",
    "        # filter the ecg\n",
    "        nyq = fs / 2\n",
    "        low = 0.5 / nyq\n",
    "        high = 40 / nyq\n",
    "        b, a = signal.butter(4, [low, high], btype='band')\n",
    "        ecg_filt = signal.filtfilt(b, a, ecg_signal)\n",
    "        \n",
    "        # normalize\n",
    "        ecg_norm = (ecg_filt - np.mean(ecg_filt)) / np.std(ecg_filt)\n",
    "        \n",
    "        # find peaks\n",
    "        min_dist = int(0.4 * fs)\n",
    "        thresh = np.mean(ecg_norm) + 0.5 * np.std(ecg_norm)\n",
    "        peaks = signal.find_peaks(ecg_norm, height=thresh, distance=min_dist)[0]\n",
    "        \n",
    "        if len(peaks) < 2:\n",
    "            return None\n",
    "        \n",
    "        # get RR intervals in ms\n",
    "        rr = np.diff(peaks) / fs * 1000\n",
    "        \n",
    "        return rr\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_wesad_subject(subject_path):\n",
    "        \"\"\"Load one subject\"\"\"\n",
    "        with open(subject_path, 'rb') as f:\n",
    "            data = pickle.load(f, encoding='latin1')\n",
    "        \n",
    "        # get chest data\n",
    "        if 'signal' not in data or 'chest' not in data['signal']:\n",
    "            print(f\"No chest data in {subject_path}\")\n",
    "            return None\n",
    "        \n",
    "        chest = data['signal']['chest']\n",
    "        labels = data['label']\n",
    "        \n",
    "        # get ecg - usually first channel\n",
    "        if isinstance(chest, dict):\n",
    "            ecg = chest['ECG'].flatten() if 'ECG' in chest else None\n",
    "        else:\n",
    "            ecg = chest[:, 0] if len(chest.shape) > 1 else chest\n",
    "            \n",
    "        if ecg is None:\n",
    "            return None\n",
    "        \n",
    "        fs = 700  # sampling rate\n",
    "        \n",
    "        return ecg, labels, fs\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_wesad_data(wesad_directory):\n",
    "        \"\"\"Process WESAD subjects\"\"\"\n",
    "        subject_files = glob.glob(f\"{wesad_directory}/**/S*.pkl\", recursive=True)\n",
    "        \n",
    "        print(f\"\\nFound {len(subject_files)} WESAD subjects\")\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        for subj_file in subject_files:\n",
    "            subj = Path(subj_file).stem\n",
    "            print(f\"Processing {subj}...\")\n",
    "            \n",
    "            result = WESADDataLoader.load_wesad_subject(subj_file)\n",
    "            if result is None:\n",
    "                continue\n",
    "            \n",
    "            ecg, labels, fs = result\n",
    "            \n",
    "            # process in 60 sec windows\n",
    "            win_size = 60 * fs\n",
    "            step = win_size // 2  # overlap\n",
    "            \n",
    "            for i in range((len(ecg) - win_size) // step + 1):\n",
    "                start = i * step\n",
    "                end = start + win_size\n",
    "                \n",
    "                if end > len(ecg):\n",
    "                    break\n",
    "                \n",
    "                ecg_win = ecg[start:end]\n",
    "                label_win = labels[start:end]\n",
    "                \n",
    "                # get most common label\n",
    "                vals, counts = np.unique(label_win, return_counts=True)\n",
    "                label = vals[np.argmax(counts)]\n",
    "                \n",
    "                # WESAD labels: 0=undefined, 1=baseline, 2=stress, 3=amusement, 4=meditation\n",
    "                # map to 3 classes: 0=low, 1=moderate, 2=high\n",
    "                if label == 0:\n",
    "                    continue\n",
    "                elif label == 1:  # baseline\n",
    "                    stress = 0\n",
    "                elif label == 2:  # stress\n",
    "                    stress = 2\n",
    "                elif label == 3:  # amusement\n",
    "                    stress = 1\n",
    "                elif label == 4:  # meditation  \n",
    "                    stress = 0\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # get RR intervals\n",
    "                rr = WESADDataLoader.get_rr_from_ecg(ecg_win, fs)\n",
    "                \n",
    "                if rr is None or len(rr) < 10:\n",
    "                    continue\n",
    "                \n",
    "                # calc HRV\n",
    "                sdnn, rmssd = WESADDataLoader.calculate_hrv(rr)\n",
    "                \n",
    "                if sdnn is None or rmssd is None:\n",
    "                    continue\n",
    "                \n",
    "                features.append({\n",
    "                    'subject': subj,\n",
    "                    'sdnn': sdnn,\n",
    "                    'rmssd': rmssd,\n",
    "                    'stress_class': stress,\n",
    "                    'source': 'WESAD'\n",
    "                })\n",
    "        \n",
    "        df = pd.DataFrame(features)\n",
    "        \n",
    "        print(f\"\\nWESAD data:\")\n",
    "        print(f\"  Total samples: {len(df)}\")\n",
    "        print(f\"  Low: {(df['stress_class'] == 0).sum()}\")\n",
    "        print(f\"  Moderate: {(df['stress_class'] == 1).sum()}\")\n",
    "        print(f\"  High: {(df['stress_class'] == 2).sum()}\")\n",
    "        print(f\"  SDNN range: {df['sdnn'].min():.1f} - {df['sdnn'].max():.1f}\")\n",
    "        print(f\"  RMSSD range: {df['rmssd'].min():.1f} - {df['rmssd'].max():.1f}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "class StressClassifier:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = None\n",
    "        self.features = ['sdnn', 'rmssd']\n",
    "    \n",
    "    def train(self, X, y, test_size=0.2):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # xgboost for 3 classes\n",
    "        self.model = xgb.XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            num_class=3\n",
    "        )\n",
    "        \n",
    "        self.model.fit(X_train_scaled, y_train, verbose=False)\n",
    "        \n",
    "        y_pred = self.model.predict(X_test_scaled)\n",
    "        y_proba = self.model.predict_proba(X_test_scaled)\n",
    "        \n",
    "        print(\"\\nEVALUATION\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        labels = ['Low', 'Moderate', 'High']\n",
    "        print(classification_report(y_test, y_pred, target_names=labels))\n",
    "        \n",
    "        print(f\"\\nConfusion Matrix:\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        \n",
    "        # try to get ROC-AUC\n",
    "        try:\n",
    "            auc = roc_auc_score(y_test, y_proba, multi_class='ovr', average='macro')\n",
    "            print(f\"\\nROC-AUC: {auc:.3f}\")\n",
    "        except:\n",
    "            print(\"\\nCouldn't compute ROC-AUC\")\n",
    "        \n",
    "        print(\"\\nFeature importance:\")\n",
    "        for f, imp in zip(self.features, self.model.feature_importances_):\n",
    "            print(f\"  {f}: {imp:.3f}\")\n",
    "        \n",
    "        # plot confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(cm, cmap='Blues')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xticks([0,1,2], labels)\n",
    "        plt.yticks([0,1,2], labels)\n",
    "        \n",
    "        # add numbers\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                plt.text(j, i, cm[i,j], ha='center', va='center')\n",
    "        \n",
    "        plt.savefig('confusion.png', dpi=150)\n",
    "        plt.close()\n",
    "        print(\"\\nSaved confusion.png\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def predict(self, X_new):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained!\")\n",
    "        X_scaled = self.scaler.transform(X_new)\n",
    "        preds = self.model.predict(X_scaled)\n",
    "        proba = self.model.predict_proba(X_scaled)\n",
    "        return preds, proba\n",
    "    \n",
    "    def save_model(self, model_file='stress_model.pkl', scaler_file='scaler.pkl'):\n",
    "        joblib.dump(self.model, model_file)\n",
    "        joblib.dump(self.scaler, scaler_file)\n",
    "        print(f\"\\nSaved {model_file} and {scaler_file}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Stress Classification - 3 Classes\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # paths\n",
    "    samsung_dir = r'D:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver2\\HRVModule\\XGBoost\\jsons'\n",
    "    wesad_dir = r'D:\\laiba\\Desktop\\USM\\CAT304W Drafts\\Working\\HealMind_ver2\\HRVModule\\XGBoost\\WESAD'\n",
    "    \n",
    "    use_samsung = True\n",
    "    use_wesad = True  # set False if no WESAD\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    # load samsung\n",
    "    if use_samsung:\n",
    "        print(\"\\nLoading Samsung data...\")\n",
    "        try:\n",
    "            loader = SamsungDataLoader()\n",
    "            stress_data, hrv_data, hr_data = loader.load_all_samsung_files(samsung_dir)\n",
    "            \n",
    "            if stress_data and hrv_data:\n",
    "                df_stress = loader.process_stress_data(stress_data)\n",
    "                df_hrv = loader.process_hrv_data(hrv_data)\n",
    "                df_samsung = loader.merge_stress_and_hrv(df_stress, df_hrv)\n",
    "                \n",
    "                df_samsung['source'] = 'Samsung'\n",
    "                all_data.append(df_samsung[['sdnn', 'rmssd', 'stress_class', 'source']])\n",
    "                print(f\"Added {len(df_samsung)} Samsung samples\")\n",
    "            else:\n",
    "                print(\"No Samsung data!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Samsung error: {e}\")\n",
    "    \n",
    "    # load wesad\n",
    "    if use_wesad:\n",
    "        print(\"\\nLoading WESAD data...\")\n",
    "        try:\n",
    "            wesad_loader = WESADDataLoader()\n",
    "            df_wesad = wesad_loader.process_wesad_data(wesad_dir)\n",
    "            \n",
    "            if df_wesad is not None and len(df_wesad) > 0:\n",
    "                all_data.append(df_wesad[['sdnn', 'rmssd', 'stress_class', 'source']])\n",
    "            else:\n",
    "                print(\"No WESAD data!\")\n",
    "        except Exception as e:\n",
    "            print(f\"WESAD error: {e}\")\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"\\nERROR: No data!\")\n",
    "        exit(1)\n",
    "    \n",
    "    # combine\n",
    "    print(\"\\nCombining datasets...\")\n",
    "    df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\nTotal samples: {len(df)}\")\n",
    "    for src in df['source'].unique():\n",
    "        src_df = df[df['source'] == src]\n",
    "        print(f\"\\n{src}:\")\n",
    "        print(f\"  Samples: {len(src_df)}\")\n",
    "        print(f\"  Low: {(src_df['stress_class'] == 0).sum()}\")\n",
    "        print(f\"  Moderate: {(src_df['stress_class'] == 1).sum()}\")\n",
    "        print(f\"  High: {(src_df['stress_class'] == 2).sum()}\")\n",
    "    \n",
    "    # train\n",
    "    print(\"\\nTraining model...\")\n",
    "    X = df[['sdnn', 'rmssd']].values\n",
    "    y = df['stress_class'].values\n",
    "    \n",
    "    clf = StressClassifier()\n",
    "    clf.train(X, y)\n",
    "    clf.save_model()\n",
    "    \n",
    "    print(\"\\nDONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b368380f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low HRV prediction: 2\n",
      "Probabilities: Low=0.25, Mod=0.02, High=0.73\n",
      "High HRV prediction: 0\n",
      "Probabilities: Low=0.57, Mod=0.42, High=0.01\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "model = joblib.load('stress_model.pkl')\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "# Test with EXTREME low HRV (should be high stress)\n",
    "test_low_hrv = np.array([[20, 15]])  # Very low SDNN and RMSSD\n",
    "scaled = scaler.transform(test_low_hrv)\n",
    "pred = model.predict(scaled)\n",
    "proba = model.predict_proba(scaled)\n",
    "\n",
    "print(f\"Low HRV prediction: {pred[0]}\")  # Should be 2 (high stress)\n",
    "print(f\"Probabilities: Low={proba[0][0]:.2f}, Mod={proba[0][1]:.2f}, High={proba[0][2]:.2f}\")\n",
    "\n",
    "# Test with high HRV (should be low stress)\n",
    "test_high_hrv = np.array([[150, 200]])\n",
    "scaled = scaler.transform(test_high_hrv)\n",
    "pred = model.predict(scaled)\n",
    "proba = model.predict_proba(scaled)\n",
    "\n",
    "print(f\"High HRV prediction: {pred[0]}\")  # Should be 0 (low stress)\n",
    "print(f\"Probabilities: Low={proba[0][0]:.2f}, Mod={proba[0][1]:.2f}, High={proba[0][2]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
